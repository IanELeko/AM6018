{"indexedPages":12,"totalPages":12,"version":"27","text":"Symmetry kills the square in a multifunctional reservoir computer\nCite as: Chaos 31, 073122 (2021); https://doi.org/10.1063/5.0055699 Submitted: 01 May 2021 • Accepted: 22 June 2021 • Published Online: 09 July 2021\nAndrew Flynn, Joschka Herteux, Vassilios A. Tsachouridis, et al. COLLECTIONS\nPaper published as part of the special topic on In Memory of Vadim S. Anishchenko: Statistical Physics and Nonlinear Dynamics of Complex Systems\nARTICLES YOU MAY BE INTERESTED IN\nMultifunctionality in a reservoir computer Chaos: An Interdisciplinary Journal of Nonlinear Science 31, 013125 (2021); https:// doi.org/10.1063/5.0019974\nOn explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD Chaos: An Interdisciplinary Journal of Nonlinear Science 31, 013108 (2021); https:// doi.org/10.1063/5.0024890\nIntroduction to Focus Issue: Recent advances in modeling complex systems: Theory and applications Chaos: An Interdisciplinary Journal of Nonlinear Science 31, 070401 (2021); https:// doi.org/10.1063/5.0061767\n\nChaos 31, 073122 (2021); https://doi.org/10.1063/5.0055699 © 2021 Author(s).\n\n31, 073122\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nSymmetry kills the square in a multifunctional reservoir computer\n\nCite as: Chaos 31, 073122 (2021); doi: 10.1063/5.0055699 Submitted: 1 May 2021 · Accepted: 22 June 2021 · Published Online: 9 July 2021\n\nView Online\n\nExport Citation\n\nCrossMark\n\nAndrew Flynn,1,a) Joschka Herteux,2 Vassilios A. Tsachouridis,3 Christoph Räth,2 and Andreas Amann1\n\nAFFILIATIONS\n1School of Mathematical Sciences, University College Cork, Cork T12 XF62, Ireland 2Institut für Materialphysik im Weltraum, Deutsches Zentrum für Luft-und Raumfahrt, Münchner Str. 20, 82234 Wessling, Germany 3Collins Aerospace—Applied Research and Technology, Cork T23 XN53, Ireland\nNote: This paper is part of the Focus Issue, In Memory of Vadim S. Anishchenko: Statistical Physics and Nonlinear Dynamics of Complex Systems. a)Author to whom correspondence should be addressed: andrew_ﬂynn@umail.ucc.ie\n\nABSTRACT\nThe learning capabilities of a reservoir computer (RC) can be stifled due to symmetry in its design. Including quadratic terms in the training of a RC produces a “square readout matrix” that breaks the symmetry to quell the influence of “mirror-attractors,” which are inverted copies of the RC’s solutions in state space. In this paper, we prove analytically that certain symmetries in the training data forbid the square readout matrix to exist. These analytical results are explored numerically from the perspective of “multifunctionality,” by training the RC to specifically reconstruct a coexistence of the Lorenz attractor and its mirror-attractor. We demonstrate that the square readout matrix emerges when the position of one attractor is slightly altered, even if there are overlapping regions between the attractors or if there is a second pair of attractors. We also find that at large spectral radius values of the RC’s internal connections, the square readout matrix reappears prior to the RC crossing the edge of chaos.\n© 2021 Author(s). All article content, except where otherwise noted, is licensed under a Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/). https://doi.org/10.1063/5.0055699\n\nSymmetry is a notorious issue in the training of a reservoir computer (RC). For example, when training some RC designs to mimic trajectories on a given attractor, a “mirrored” version of the attractor is automatically created in the RC’s state space due to symmetry. Fortunately, we can suppress the influence these “mirror-attractors” exert on the RC’s learning capacity by breaking the symmetry. In this paper, we investigate the behavior of the “square readout matrix,” which is produced to destroy the mirror-attractors by including quadratic terms in the training of the RC’s readout layer. We consider these symmetry associated phenomena from the perspective of “multifunctionality” by specifically training the RC to reconstruct a coexistence of attractors related by symmetry. We prove analytically that under certain conditions, symmetries in the training data prohibit the square readout matrix from existing. The consequences of these analytical results are explored in various numerical experiments. We examine the behavior of the square readout matrix when the RC is trained to exhibit multifunctionality by reconstructing a\n\npair of mirrored-Lorenz attractors. Changing the location of one Lorenz attractor results in the square readout matrix appearing. This effect persists even in the more complex cases of overlapping regions between the pair of Lorenz attractors or the coexistence of two mirror-attractor pairs. We also show that at critical spectral radius values of the RC’s internal layer, the square readout matrix appears even when driving the RC with a single source of symmetrical input training data.\nI. INTRODUCTION\nDespite the growing number of machine learning applications, many of these methods are considered to be “black-boxes,” which oftentimes results in very little being understood about how the machine actually learns. Fortunately, reservoir computing1–3 is one machine learning approach that can be rigorously assessed through the lens of dynamical systems.4–8\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-1\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nA reservoir computer (RC) can be realized as a type of artificial neural network consisting of three layers: input, internal, and readout layer. If the internal layer, known as the “reservoir,” is suitably designed, then only the weights of the readout layer need to be determined in order to train the RC. Nonlinear time-series prediction and attractor reconstruction are two of the most well known application areas of RCs.9–13\nThere are many ways to design the RC’s readout layer with respect to a given problem. While originally it was mostly linear terms that were used to train the readout layer, in recent years, the inclusion of quadratic terms in the training to produce a “square readout matrix” has become popular after its introduction by Lu et al.,14 where it was employed to handle symmetries of the Lorenz system.15 This “squaring technique” was empirically found to improve a RC’s performance in more general cases.11,16 This can, in part, be attributed to the ability of the square readout matrix to break harmful symmetries present in the equations describing a RC.17 In the context of attractor reconstruction, these harmful symmetries must be broken in order to prevent the creation of a “mirror-attractor,” an inverted version of the attractor that the RC was specifically trained to reconstruct.\nIn this paper, we consider these phenomena associated with symmetry from the perspective of “multifunctionality.” This neuroscientific term describes a neural network with the ability to perform more than one task without changing any synapses. There are numerous examples of multifunctionality found to occur in nature; for further reading, we suggest Ref. 18, and references therein. Recently Flynn et al.19 translated multifunctionality to a machine learning setting. Here, it was demonstrated that a RC can be trained to create a coexistence of multiple attractors from different dynamical systems. The multifunctional RC is directly analogous to its biological counterparts as it can perform different tasks depending on a given initial condition. The same result has also been shown by Herteux and Räth17 with a similar training technique.\nBy including certain symmetries in the training data, we are able to “open the black-box” and expose some of the fundamental properties of a multifunctional RC. In Sec. III, we prove analytically that a point or inversion symmetry of the training data necessitates a disappearance of the square readout matrix in order for the RC to exhibit multifunctionality or reconstruct a single anti-symmetric trajectory.\nIn Sec. IV, we use the analytical results from Sec. III as a platform to extend our study of the relationship between symmetry and multifunctionality to several numerical experiments. We focus on training the RC to specifically reconstruct a coexistence of the Lorenz attractor and its mirror-attractor. It is only when the symmetry in the training data is broken that we see a contribution from the square readout matrix.\nWe also show that symmetry still “kills the square” for two more complex examples. In the first case, we manipulate the training data from the Lorenz attractor such that the trajectories appear to intersect with its mirror-attractor in state space. We train the RC to reconstruct a coexistence of these overlapping attractors and show that it is only when the location of one attractor is slightly shifted that the square readout matrix appears. In the second case, we investigate the behavior of the square readout matrix when there are overlapping regions between two pairs of mirrored-Lorenz\n\nattractors. We show that it is sufficient to break the symmetry between only one pair of attractors for the square readout matrix to emerge.\nUsing a simplistic example of anti-symmetric training data, we find that at a certain spectral radius value of the RC’s internal connections, the square readout matrix appears. By slightly increasing the spectral radius past this critical point, the elements of the matrix grow in magnitude as chaos begins to conquer the RC dynamics.\nThe rest of the paper is outlined as follows: in Sec. II, we introduce the type of RC we consider in this paper and the particular squaring technique we use to break the symmetry, and the role of symmetry in the RC is discussed in greater length. The specifics of training the RC to achieve multifunctionality are also outlined. In Sec. III, we present our analytical results that are explored numerically in Sec. IV. We provide some concluding remarks in Sec. V.\n\nII. RESERVOIR COMPUTING\nEcho-State Networks (ESNs)2 and Liquid-State Machines (LSMs)3 are two independently proposed designs of “recurrent neural networks” that are the foundation of reservoir computing.1 Central to the philosophy behind this machine learning approach is that instead of training all the weights in a recurrent neural network, it is sufficient to optimize only the weights of a readout layer. This ideological shift stems from the design of a suitable internal layer, known as the “reservoir,” which does not need to be trained according to a given task. Many physical mediums can play the role of a reservoir.20,21 In this paper, we construct the reservoir as a network of artificial neurons with a sparse Erdös–Renyi topology. We make use of the continuous-time formulation devised by Lu et al.,9 which is expressed in the following equation:\n\nr˙(t) = γ −r(t) + tanh ( M r(t) + σ Win u(t) ) .\n\n(1)\n\nHere, r(t) ∈ RN is the state of the RC at a given time t and N is\nthe number of neurons in the network. γ is the decay-rate parameter. M ∈ RN×N is the adjacency matrix describing the reservoir. σ is the input strength parameter and Win ∈ RN×D is the input matrix; when multiplied together, this represents the weight given to the Ddimensional input time-series, u(t) ∈ RD, as it is projected into the\nreservoir. Solutions of Eq. (1) are computed using the fourth order Runge–Kutta method with time step τ = 0.01. In the Appendix, we\noutline how M and Win are designed. A key parameter involved in this RC setup is the spectral radius,\nρ, of the internal connection, M. ρ is associated with the RC’s mem-\nory as it is used to change the weight the RC places on its own\ninternal dynamics. In Sec. IV, ρ plays a key role in several instances. The RC in Eq. (1) is driven by the input u(t) from t = 0 to\ntime t = tlisten in order to remove any dependency r(t) has on its initial condition r(0) = (0, 0, . . . , 0)T = 0T. The training data are generated by continuing to drive the RC with u(t) from t = tlisten to t = ttrain.\nA suitable readout layer needs to be calculated in order to train\nthe RC. We replace the training input signal, u(t), in Eq. (1) with a post-processing function, ψˆ (·). If the training is successful, then we\nsay that\n\nψˆ (r(t)) = uˆ (t) ≈ u(t) for t > ttrain,\n\n(2)\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-2\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nwhere uˆ (t) is the predicted time-series. This layer closes the loop of the nonautonomous equation (1) and provides a map from the high-dimensional state space of the RC, S, to the D-dimensional “prediction state space,” P.\nIn this paper we choose ψˆ (r(t)) = Woutq(r(t)) where Wout is the readout matrix and we use q(r(t)) to break the symmetry using the “squaring technique” described by\n\nq(r(t)) =\n\nr(t) r2(t)\n\n.\n\n(3)\n\nWe then write\n\nψˆ (r(t)) = Woutq(r(t)) = W(o1u)t W(o2u)t\n\nr(t) r2(t)\n\n= W(o1u)t r(t) + W(o2u)t r2(t),\n\n(4)\n\nwhere W(o1u)t is the linear readout matrix and W(o2u)t is the square readout matrix.\nTo calculate Wout, we use a ridge regression given by\n\nWout = YXT XXT + β I −1 ,\n\n(5)\n\nwhere\n\nX=\n\nr(tlisten) r2 (tlisten )\n\nr(tlisten + τ ) r2(tlisten + τ )\n\n···\n\nr(ttrain) r2(ttrain)\n\n(6)\n\nis the response of the RC to the input data\n\nY = u(tlisten) u(tlisten + τ ) · · · u(ttrain) .\n\n(7)\n\nβ is the regularization parameter that is used to discourage overfitting and I is the identity matrix.\nWe then write the “predicting RC” as\n\nr˙ˆ(t) = γ − rˆ(t) + tanh M rˆ(t) + σ WinW(o1u)trˆ(t)\n\n+ σ WinW(o2u)trˆ2(t) ,\n\n(8)\n\nwhere rˆ(0) = r(ttrain).\n\nA. Symmetry\n\nAn important property to consider when designing a RC is the symmetry of the equations as it will usually harm the RC’s learning capability. For example, if we simply choose q(rˆ(t)) = rˆ(t) in our setup, we get a common symmetry as the equation\n\nr˙ˆ(t) = γ −rˆ(t) + tanh M rˆ(t) + σ WinWout rˆ(t))\n\n(9)\n\nis invariant under inversion of the sign rˆ(t) → −rˆ(t). This results in a symmetric partner for every solution of Eq. (9), whereby for any attractor, A, in S, there is a corresponding “mirror-attractor,” A . Additionally, the linear readout results in pairs of attractors and mirror-attractors appearing in P.\nThis symmetry induced phenomenon and, in particular, how to overcome the issues related to it was explored in great detail by Herteux and Räth.17 Here, it was proven analytically that changing the sign of the input and target data in the listening stage also leads\n\nto a sign change in the resulting reservoir states used as training data given the reservoir has the echo state property. Because of this, the calculation of the readout matrix Wout will give the same weights. It was also highlighted that symmetry becomes most problematic if the location of the training data is not chosen correctly as the attractors in the S may merge with one another.\nThere are numerous ways to break this symmetry to mitigate the restraints it imposes, for example, using a bias in the output, a shift in the input, or including square terms in the readout.17 In this paper, we focus on the square terms in the readout described by Eq. (3). In this case, we see that the RC is not invariant under the change of sign like before as\n\nψˆ −rˆ(t) ≈ W(o1u)t −rˆ(t) + W(o2u)t −rˆ(t) 2\n\n(10)\n\n= −W(o1u)t rˆ(t) + W(o2u)t rˆ2(t) = −ψˆ rˆ(t) .\n\n(11)\n\nW(o2u)t rˆ2(t) breaks the symmetry and the mirror-attractors are destroyed. In the current paper, we consider this phenomenon from the perspective of multifunctionality and find that the opposite is also true. More specifically, in Sec. III, we prove analytically that W(o2u)t = 0 when there are certain symmetries present in the training data. In Sec. IV, we show that when the RC in Eq. (1) is trained using the squaring technique in Eq. (3) to reconstruct a coexistence of the Lorenz attractor and its mirror-attractor, then W(o2u)t does not come into existence.\nWe remark that the effect of symmetries in reservoir computing has been investigated in a number of further previous studies. Carroll and Pecora22 show how symmetries in the structure of the network lead to a lower covariance rank and negatively affect the training error. A permutation symmetry in the reservoir states was found to severely impact the performance of a swarm based RC in Ref. 23. On the other hand, matching the symmetry of the input data with a suitable handcrafted RC design was shown to cause drastic improvements in two tasks by Barbosa et al.24 In this work, we examine how a RC can adapt to and take on the symmetry of the trained system spontaneously.\nNext, we outline how to train a RC to exhibit multifunctionality.\n\nB. Multifunctionality\nA multifunctional neural network can perform multiple tasks by changing its activity patterns according to a particular input without the need of changing any connections. Multifunctionality is prevalent in networks that are used to switch between mutually exclusive tasks, for example, swimming or crawling,25,26 and regular breathing or sighing or gasping.27\nTranslating multifunctionality to a RC is the same as saying that the RC must be trained to reconstruct the behavior of more than one attractor using the same readout layer.19 In other words, if for a given M and Win a post-processing function, ψˆ (·), can be found such that\n\nψˆ rˆA(t) ≈ uA(t) and ψˆ rˆB(t) ≈ uB(t)\n\n(12)\n\nfor t > ttrain, then we say that the RC is multifunctional as a single network can successfully reproduce the long term behavior of\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-3\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\ntwo different time-series uA and uB based on a particular initial condition.\nIn order to train the RC to exhibit multifunctionality, we use the “blending technique” presented in Ref. 19. The first step of this technique is to drive the reservoir in Eq. (1) with uA(t) from t = 0 to t = ttrain and collect the corresponding training data in the matrices XA and YA like in Eqs. (6) and (7). We then repeat this process for the other input source uB to obtain XB and YB. Both reservoir training data matrices are then concatenated in the following way:\n\nXC = (αXA, (1 − α)XB) .\n\n(13)\n\nThe same process is applied to construct the equivalent input concatenation matrix, YC. The parameter α ∈ [0, 1] “blends” two different sources of data together by applying more weight to one set of data over the other. This proved particularly useful when training a RC to reconstruct a coexistence of chaotic attractors with different dynamical characteristics.19 We take the additional step to randomly reorder each column of the matrices, XC and YC, corresponding to the input and reservoir state at a given time. The matrices, XC and YC, are then used in the ridge regression formula in Eq. (5).\nOnce the training is successful, if the predicting RC in Eq. (8) is initialized with either rˆA(0) or rˆB(0), then the RC will predict the future evolution of uA(t) or uB(t) from t = ttrain.\nWe remark that similar results exist in the literature regarding RCs and the learning of multiple attractors. For example, Inoue et al.28 and Lu and Bassett29 have shown that a RC can switch between learning to reconstruct different periodic and chaotic attractors using an online training scheme. Furthermore, Ceni et al.30 demonstrated that an external input can be used to force a RC to switch between different fixed points.\nThe key difference between these results and multifunctionality is that a single readout matrix is calculated to create multistability of attractors in the RC’s high-dimensional state space, S, that resemble the target behavior when projected to the lower-dimensional prediction state space, P.\n\nIII. ANALYTICAL RESULTS\n\nA. Anti-symmetric training data leads to W(o2u)t = 0\n\nLet us now study the properties of the ridge regression formula in Eq. (5) if it is presented with anti-symmetric data. Let us assume that X and Y both have 2n columns and are obtained from data ri and ui with the property\n\nui+n = −ui,\n\n(14)\n\nri+n = −ri.\n\n(15)\n\nThen, we have\n\nX=\n\nr1 r21\n\n···\n\nrn r2n\n\n−r1 r21\n\n···\n\n−rn r2n\n\n,\n\n(16)\n\nY = u1, · · · , un, −u1, · · · , −un .\n\n(17)\n\nThis means that\n\nYXT = 2\n\nn i=1\n\nui\n\nrTi ,\n\n0,\n\n(18)\n\nXXT = 2\n\nn i=1\n\nrirTi\n\n0\n\n2\n\n0\n\nn i=1\n\nr2i\n\nr2i T\n\n.\n\n(19)\n\nPlugging this into Eq. (5), we obtain\n\nWout = 2\n\nn i=1\n\nuirTi\n\n2\n\nn i=1\n\nri rTi\n\n+ βI\n\n−1 ,\n\n0.\n\n(20)\n\nTherefore, whenever the training data fulfills the conditions in Eqs. (14) and (15), then it follows that\n\nW(o2u)t = 0.\n\n(21)\n\nB. Sources of anti-symmetric training data\n\n1. Multiple trajectories related by symmetry\n\nNow that we know that anti-symmetric training data lead to a vanishing W(o2u)t = 0, and we would like to investigate under what conditions such anti-symmetric training data naturally arise. In the\ncontext of multifunctionality, this can happen if we train the system\nwith two separate trajectories from two attractors u1(t) and u2(t), which are related by\n\nu1(t) = −u2(t).\n\n(22)\n\nTime discretization and concatenation of the two trajectories according to the blending technique of Eq. (13) with α = 1/2 then immediately shows that Eq. (14) holds. To show that Eq. (15) is also fulfilled, we remember that both r1(t) and r2(t) are obtained from Eq. (1). Let us define\n\nf(r, u) = γ −r + tanh (Mr + σ Winu) = −f(−r, −u), (23)\n\nwhere in the second equation, we observe that f is anti-symmetric.\nIf now r1(t) is a solution of Eq. (1) for some initial condition r1(0) then r2(t) = −r1(t) is a solution for initial condition\n\nr2(0) = −r1(0).\n\n(24)\n\nThis is demonstrated by\n\nr˙2(t) = −r˙1(t) = −f(r1(t), u1(t))\n\n(25)\n\n= −f(−r2(t), −u2(t)) = f(r2(t), u2(t)).\n\n(26)\n\nTherefore, Eq. (15) holds. This means that if the system is trained\nwith two distinct anti-symmetric trajectories and the initial condition of the reservoir state is anti-symmetric, the resulting W(o2u)t matrix automatically vanishes. This result can be easily generalized\nto any even number of distinct trajectories uk(t), which are pairwise anti-symmetric,\n\nu2k(t) = −u2k+1(t).\n\n(27)\n\n2. Single anti-symmetric trajectory\n\nAnother source of anti-symmetric data is the case of periodic trajectories of period T, which are of the form\n\nu\n\nt+ T 2\n\n= −u(t).\n\n(28)\n\nThe reservoir trajectory r(t) is the response of the system to the periodic drive u(t) according to Eq. (1). By the symmetry, there always\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-4\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nTABLE I. Hyperparameters and time parameters used to generate the results displayed in each of the speciﬁed ﬁgures. T is the period of the circular orbit described in Eq. (31). These parameters were chosen using a grid search method.\n\nFigure\n\nρ\n\nσγ\n\nβ\n\nα tlisten ttrain tpredict\n\n1 and 2 0.2\n\n0.2 10 10−2 0.5 100 100 100\n\n5\n\n1.7\n\n0.3 5 10−1 0.5 100 100 100\n\n6\n\n1.7 0.22 5 10−1 n/a 100 100 100\n\n7 and 8 [1.4, 2.4] 0.3 10 10−2 0.5 30T 30T 30T\n\nexists a solution of the form\n\nr\n\nt\n\n+\n\nT 2\n\n= −r(t).\n\n(29)\n\nHowever, this solution is not necessarily stable. In our numerical experiments in Sec. III B, we find that the solution is stable up to a certain value of ρ and in this case, Eq. (15) is fulfilled, which again gives rise to vanishing W(o2u)t.\nWe remark that there is scope to extend the results in this section to the other symmetry breaking terms identified in Ref. 17 and we leave this for future work.\nWe now investigate how these analytical results can be realized in different numerical experiments.\n\nIV. NUMERICAL RESULTS\nIn this section, we discuss the results of our numerical experiments regarding symmetry and multifunctionality. The RC hyperparameters used to produce each of the figures are given in Table I. Also, note that in each case the predicted trajectory for a given input/attractor, A, is represented by Aˆ, which is in keeping with the notation provided in Sec. II.\n\nA. Multifunctionality with mirrored-Lorenz attractors\nFirst, we consider the case described in Eq. (22) by training the RC in Eq. (1) with the squaring technique in Eq. (3) to reconstruct the chaotic Lorenz attractor, L, and its mirror-attractor, L . The input data are generated using the fourth order Runge–Kutta method with time step τ = 0.01. The mirror-attractor is created by taking the negative of the Lorenz input time-series, uL(t) = x(t), y(t), z(t) T, where x(t), y(t), and z(t) are the dynamical variables of the Lorenz system.15\nWe use the methods outlined in Sec. II B to calculate Wout = W(o1u)t W(o2u)t in order to train the RC to exhibit multifunctionality\nby reconstructing either L or L based on a given initial condition. The result of this is shown in Figs. 1(a) and 1(b).\nIn Fig. 1(a), we plot the reconstructed attractors, Lˆ and Lˆ , in the prediction state space P along with the target attractors L and L . In this case, we see that multifunctionality was achieved by the RC as both attractors were successfully reconstructed using the same Wout matrix. However, in Fig. 1(b), we see that despite breaking the symmetry in the training by using the squaring technique defined in Eq. (3), the square readout matrix, W(o2u)t, does not come into existence. In keeping with the analytical results in Sec. III,\n\nthe symmetry in the training data forces the RC to also become\nsymmetric.\nNext, we consider breaking the symmetry in the training\ndata by shifting L by a factor χ in the x-direction, i.e., uLχ (t) = −x(t) − χ , −y(t), −z(t) T. In Figs. 1(c) and 1(d), we display the\nresults for χ = 2.\nIn Fig. 1(c), we see that the RC exhibits multifunctionality as\nit can reconstruct both L and Lχ using the same readout matrix. However, we see in Fig. 1(d) that once the symmetry in the training\ndata is broken by slightly shifting the location of the mirror-attractor in state space, then W(o2u)t comes into existence.\nWe now explore what happens to W(o2u)t when shifting the location of the mirror-attractor for χ ∈ [−5, 5]. In Fig. 2, we plot histograms of W(o2u)t as a function of χ . Here, each color represents the number of W(o2u)t elements at a specific value, and the more elements there are at a certain value, the darker the color is. For χ = 0, we see the known result from Fig. 1(b) where all elements of W(o2u)t equal to 0. We see that the elements of W(o2u)t grow larger as the magnitude of χ is increased. So, we can say that the more we break the\nsymmetry in the training data, the greater the contribution from W(o2u)t in the dynamics of the multifunctional RC.\nThe conditions defined in Eqs. (14) and (15) rely on the\nassumption that both training data sets have precisely the same number of points. We now explore the behavior of W(o2u)t as the RC is trained with input from trajectories on L and L using the respective\ntraining times tLtrain and tLtrain. The resultant imbalance in the data sets is characterized by the parameter κ ∈ [0, 1] defined as\n\nκ\n\n=\n\ntLtrain tLtrain + tLtrain\n\n.\n\n(30)\n\nTherefore, when κ = 0.5, the conditions in Eqs. (14) and (15) are satisfied as tLtrain = tLtrain, which results in W(o2u)t = 0. In Fig. 3, we illustrate the behavior of W(o2u)t for κ ∈ [0, 1] and tLtrain + tLtrain = 200. We observe that for a small change in κ away from 0.5, the histogram of W(o2u)t elements is still strongly peaked at zero. To provide further details on the changes of W(o2u)t away from κ = 0.5, we plot the cumulative density function of the absolute values of the elements of W(o2u)t for a number of κ values in Fig. 4. This shows that for κ = 0, which corresponds to the case of training the RC with data only from L, roughly 50% of the magnitude of W(o2u)t element values are greater than 0.08. However, when 20% of the total amount of\ntraining data is taken from L , i.e., κ = 0.2, then 50% of the magnitude of W(o2u)t element values are less than 0.03. This means that including only a modest amount of data from L during the training reduces the magnitude of W(o2u)t element values by nearly a factor of 3. This trend continues as κ is further increased, and for κ = 0.48,\nwhich corresponds to a slight imbalance in the amount of training data, more than 50% of W(o2u)t elements have a magnitude of less than 0.002. While W(o2u)t completely vanishes only when κ = 0.5, it is still strongly suppressed even when there is an imbalance in the amount\nof training data.\nIn Fig. 3, we also highlight the range of κ values (0.1 ≤ κ ≤ 0.9)\nin which the RC is able to achieve multifunctionality. This indicates\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-5\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 1. Illustrating the behavior of W(o2u)t when the RC is trained to exhibit multifunctionality in cases of mirror-attractors. (a) Reconstruction of the Lorenz attractor, L and its mirror-attractor L and in (b) color plot of the corresponding trained Wout matrix individual elements. Each of the i rows in Wout is denoted by the variable it reconstructs. Each column represents the weight, wi,j, given to the jth component of q(r(t)) = (r(t), r2(t))T . The distinction is made between which columns belong to W(o1u)t and W(o2u)t . (c) Reconstruction of L and the slightly shifted Lχ and in (d) the corresponding trained Wout matrix.\n\nthat even when there is a strong imbalance in the training data used from each attractor, the RC is still able to exhibit multifunctionality.\nB. Multifunctionality with overlapping attractors\nSymmetry in the RC equations can oftentimes lead to catastrophic consequences once the training data intersect with its mirrored version. This symmetry induced phenomenon was discussed in Ref. 17. In this paper, we consider this problem from the perspective of multifunctionality.\nBy introducing overlapping regions between the different sources of input data, it becomes much more evident what needs\n\nto happen in order for a RC to achieve multifunctionality. In Sec. II B, we say that a multifunctional RC creates multistability of attractors in S such that when these attractors are projected to P using Wout, they resemble the target behavior. We now explore this numerically.\nIn this section, we investigate the behavior of W(o2u)t in two different cases of overlapping attractors. First, we focus on the scenario where the RC is trained to reconstruct a coexistence of the Lorenz attractor and its mirror-attractor, which share mutual regions of state space. We then consider training the RC to reconstruct two pairs of overlapping mirrored-Lorenz attractors.\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-6\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 2. Histograms of Wo(2u)t as a function of χ .\n1. One pair of mirrored-Lorenz attractors In order to produce the overlapping regions in the data,\nwe simultaneously shift the location of both L and L along the z-direction by a factor ζ in order to preserve the symmetry. In other words, we use uLζ (t) = x(t), y(t), z(t) + ζ T and uLζ (t) = −uLζ (t) as the input data to train the RC to exhibit multifunctionality. The result of this for ζ = −31 is shown in Figs. 5(a) and 5(b).\nIn Fig. 5(a), we see that the RC is able to reconstruct the overlapping Lorenz attractors. The particular Wout matrix that is used to achieve multifunctionality is shown in Fig. 5(b). Here, we see that even in the case when the attractors are overlapping, the symmetry in the training data eliminates any contribution from W(o2u)t to the RC dynamics.\nWe now check if W(o2u)t will come into existence once the symmetry in the training data is broken by shifting the mirrorattractor along the x-axis by a factor χ like in Sec. IV B. In this case, we now train the RC with uLζ (t) as before and uLχ,ζ (t)\nFIG. 3. Histograms of W(o2u)t as a function of κ.\n\nFIG. 4. Cumulative density function of the magnitude of W(o2u)t element values.\n= −x(t) − χ , −y(t), −z(t) − ζ T. In Figs. 5(c) and 5(d), the result of the training for χ = 8 and ζ = −31 is shown.\nFigures 5(c) and 5(d) precisely illustrate what the analytical work in Sec. III predicts. Once the symmetry in the training data is broken, then W(o2u)t exists even if there is an overlap between the different trajectories described by the training data.\nWe now investigate how this result scales to the case of four overlapping Lorenz attractors.\n2. Two pairs of mirrored-Lorenz attractors\nThere are many different ways in which we can realize the extent of our analytical results in Sec. III in a numerical setting. As long as the training data satisfy the conditions in Eqs. (14) and (15), then we see from Eq. (27) that in general “symmetry kills the square” when training the RC to reconstruct any number of pairs of mirrored-attractors.\nIn order to illustrate this numerically, we adapt the blending technique from Sec. II B to train a RC to achieve multifunctionality with more than two attractors. Instead of using the blending parameter α in the matrix concatenation step we follow the training procedure presented in Ref. 17. In this approach, the resultant XC and YC matrices that are used in the Eq. (5) are a concatenation of all the individual reservoir and input data training matrices, respectively.\nIn this section, we explore the behavior of W(o2u)t when training the RC to reconstruct two pairs of mirrored-Lorenz attractors with overlapping regions between all four attractors. We use the original Lorenz data, uL(t), to generate the training input to the RC for these four attractors. Using the shifting parameters, χ and ζ , we write the input from the four Lorenz attractors as uL1 = x(t) + χ , y(t), z(t) + ζ T, uL1 = −uL1 , uL2 = −x(t) − χ , −y(t), z(t) + ζ T, and uL2 = −uL2 . The result of this for χ = −15 and ζ = −40 is illustrated in Figs. 6(a) and 6(b).\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-7\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 5. Illustrating the behavior of W(o2u)t when the RC is trained to exhibit multifunctionality in cases of overlapping mirror-attractors. (a) Reconstruction of the overlapping Lζ and its mirror-attractor Lζ and in (b) plot of the corresponding trained Wout matrix. (c) Reconstruction of Lζ and the slightly shifted Lχ,ζ and in (d) the corresponding trained Wout matrix. Description of Wout same as in Fig. 1.\n\nIn Fig. 6(a), we see that the RC exhibits multifunctional-\nity by successfully reconstructing each of the four Lorenz attrac-\ntors. The analytical results from Sec. III continue to hold as we see from Fig. 6(b) that W(o2u)t is prohibited from contributing to the prediction due to the symmetry between each indi-\nvidual pair of the mirrored-Lorenz attractors in the training\ndata.\nTo break the symmetry, we shift the location of only L1 in the positive x and z directions. In Figs. 6(c) and 6(d), we see that despite\nthe symmetry being present for L2 and L2, it is sufficient to only break the symmetry between the other Lorenz attractors, L1 and L1, for W(o2u)t to come into existence.\n\nWe remark that our results significantly depend on an appropriate choice of the spectral radius, ρ. The state of the RC must have sufficient knowledge of its previous states in order to avoid spontaneously switching over to any of the other attractors when approaching a region of overlap. By increasing the value of ρ from 0.2 in Sec. IV A to 1.7 in this current section, we found that the RC was then able to distinguish between the different input data sources in order to exhibit multifunctionality. When increasing the amount of overlap between the attractors, the RC required larger values of ρ in order to achieve multifunctionality. On the other hand, if ρ is too large, then there is a danger that the RC can be pushed beyond the edge of chaos.5\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-8\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 6. Illustrating the behavior of W(o2u)t when the RC is trained to exhibit multifunctionality in cases of two pairs of overlapping mirror-attractors. (a) Reconstruction of the overlapping L1, L1, L2, and L2 and in (b) plot of the corresponding trained Wout matrix. (c) Reconstruction of overlapping L1, L1, L2, and the slightly shifted L2 and in (d) the corresponding trained Wout matrix. Description of Wout same as in Fig. 1.\n\nHowever, studying the relationship between ρ, overlapping\ndata, and number of attractors goes beyond the current scope of this\npaper and we choose to leave this for future work.\nNext, we investigate the analytical results in Sec. III B 2 using a paradigmatic example that shows that at large values of ρ, W(o2u)t appears despite training the RC using an anti-symmetric input.\n\nthe case where Eq. (14) holds but Eq. (15) does not at critical values of the spectral radius, ρ.\nWe focus on the case outlined in Sec. III B where a single source of input training data is already anti-symmetric as described by Eq. (28). We construct this scenario numerically using the following source of periodic training data:\n\nC. Wo(2u)t reappears at large ρ\nSo far, in our numerical experiments, we have only been able to observe the appearance of W(o2u)t when both Eqs. (14) and (15) are violated. In this section, we show that W(o2u)t can come into existence in\n\nuC(t) =\n\n5 cos t 5 sin t\n\n,\n\n(31)\n\nwhich describes an orbit on a circle of radius 5 rotating anticlockwise at constant angular velocity with period T = 2π .\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-9\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 7. Histograms of W(o2u)t as a function of ρ. Also plotted is the largest Lyapunov exponent of the predicting RC in Eq. (8), λmax, as a function of ρ.\nThis input has the symmetry described in Eq. (28). Therefore, when uC(t) is used to drive the RC in Eq. (1) and the state of the RC fulfills the condition in Eq. (29), then Eqs. (14) and (15) are satisfied. So, W(o2u)t is prevented from coming into existence.\nTuning the spectral radius, ρ, is crucial to a successful training outcome. We find in our numerical experiments that if ρ is too large, then Eq. (29) does not hold despite Eq. (28) being fulfilled. We now explore the behavior of W(o2u)t when the RC is trained with uC(t) as input to Eq. (1) for ρ ∈ [1.4, 2.4].\nIn Fig. 7, we plot a histogram of the W(o2u)t matrix elements as a function of ρ. The darker the color each point is in Fig. 7, the more there are elements of W(o2u)t at a given value. Also included in Fig. 7 is the largest Lyapunov exponent of the predicting RC in Eq. (8), λmax, as a function of ρ.\nIn this picture, we see that for ρ < ρs ≈ 1.715, all elements of W(o2u)t are 0 and λmax = 0, indicating that the RC obeys the symmetry conditions specified in Sec. III B. However, we see that for ρs < ρ < ρc = 1.78, a small number of W(o2u)t elements are now nonzero. At the same time, λmax ≈ 0 within this range of ρ values. For ρ > ρc,\n\nthe RC goes beyond the edge of chaos and as λmax increases from 0, we see the elements of W(o2u)t begin to grow much faster.\nThe RC’s predicted trajectory on C at each of these stages, ρ < ρs, ρs < ρ < ρc, and ρ > ρc is shown in Fig. 8.\nHere, we see for increasing values of ρ, the reconstruction of C goes from good, to bad, to ugly. For ρ = 1.6, the RC can successfully reconstruct trajectories on C. However, when ρ = 1.75, the reconstruction of C as viewed in P is neither chaotic nor symmetric. The RC fails to reconstruct trajectories on the cycle C for ρ = 2.2.\nWe remark that the figures provided in this section correspond to one sample of 100 different random realizations of M and Win. Qualitatively, the behavior was relatively similar for all instances. This step was taken to ensure that our results are not an artifact of a particular initialization of the RC but are direct characteristics of the RC itself.\nV. CONCLUSION\nBy including certain symmetries in the training data, we are able to “open the black-box” and provide a greater understanding of how a RC learns to perform certain tasks in addition to exposing some of the underlying phenomena that arise when translating multifunctionality to a RC.\nBreaking the symmetry intrinsic to certain formulations of a RC is necessary in order to mitigate the influence of “mirrorattractors” on the RC’s learning capacity.17 We focus on the “squaring technique,” introduced in Ref. 14 and described in Eq. (3). This approach results in the creation of a “square readout matrix,” W(o2u)t, that breaks the symmetry in the RC’s readout layer and destroys the mirror-attractors.\nIn this paper, we explore the behavior of W(o2u)t in the case where there are already certain symmetries present in the training data. In Sec. III, we prove that when the RC is trained to reconstruct anti-symmetric training data when using the ridge regression technique described in Eq. (5), it then follows that W(o2u)t must vanish. We provide examples in Eqs. (22), (27), and (28) of anti-symmetric training data that can under certain conditions “kill the square.” In Sec. IV, we explore each of these examples in a numerical setting. We\n\nFIG. 8. Prediction of the circular trajectory C. (a) ρ = 1.6, (b) ρ = 1.75, and (c) ρ = 2.2.\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-10\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nconsider the cases described in Eqs. (22) and (27) from the perspective of multifunctionality. More specifically, we investigate the behavior of W(o2u)t when the RC is trained to reconstruct a coexistence of the Lorenz attractor and its mirror-attractor. We show in Fig. 1 that when the location of one attractor is slightly shifted, which thereby conflicts with the conditions in Eqs. (14) and (15), only then does W(o2u)t appear. We see the same effect occur in Fig. 5 with the added complication of overlapping regions between the pair of mirrored-Lorenz attractors. This result demonstrates that multifunctionality can be used to overcome the harmful properties of symmetry as described in Ref. 17.\nThe results in Figs. 2–4 demonstrate that the conditions defined in Eqs. (14) and (15) are robust to small changes in both χ from 0 and κ from 0.5 as the majority of W(o2u)t element values remain relatively close to 0. However, the more we change χ and κ the larger the elements of W(o2u)t become.\nTo explore the square killing scenario described by Eq. (27) in a numerical setting, we consider training the RC to reconstruct two pairs of mirrored-Lorenz attractors in Sec. IV B 2. In Fig. 6, we demonstrate that it is sufficient to break the symmetry between one pair of mirrored-Lorenz attractors in order for W(o2u)t to come into existence.\nIn Sec. IV C, we construct the case of a single anti-symmetric trajectory considered in Eq. (28) by using a circular orbit described by Eq. (31). Here, we demonstrate that when the RC is successfully able to reconstruct a trajectory on this circular orbit, there is no contribution from W(o2u)t. However, if the spectral radius of the RC’s internal connections, ρ, is too large, then the prediction breaks down as the RC begins to exhibit chaos and W(o2u)t emerges.\nThis paper demonstrates the benefits of studying the effects of symmetries in a RC setup and serves as a route to better understand the fundamentals of this machine learning paradigm which we intend to further pursue. Overall, the two-armed approach of our analytical results together with our numerical experiments provide a much greater insight into the relationship between symmetry and multifunctionality in a RC. The inclusion of exotic cases like overlapping regions between attractors in the prediction state space and achieving multifunctionality with four attractors further increase the robustness of our results and showcase some of the interesting dynamics a RC can be trained to simultaneously reconstruct. We identify that there is a connection between these newly demonstrated abilities of a multifunctional RC and the spectral radius, ρ. However, exploring the important role of ρ in these scenarios goes beyond the scope of the current paper and we choose to leave this for future work.\nACKNOWLEDGMENTS\nA.F. is funded by the Irish Research Council Enterprise Partnership Scheme (Grant No. EPSPG/2017/301). We wish to acknowledge Andrea Ceni for his helpful comments.\nAPPENDIX: RC DESIGN AND TRAINING PARAMETERS\nIn all of our numerical simulations, we set the number of neurons in the reservoir as N = 1000. The neurons are connected\n\nwith a sparse Erdös–Renyi topology. In this case, the adjacency matrix, M, is designed such that each element is chosen independently to be nonzero with probability P = 0.04 and these nonzero elements are chosen uniformly from (−1, 1). This random sparse matrix is then rescaled such that it has a specific spectral radius, ρ. The elements of the input matrix, Win, are also chosen randomly such that each row has only one nonzero randomly assigned element, chosen uniformly from (−1, 1).\nThe numerical results illustrated in Figs. 1–6 are generated using the same M and Win in order to provide consistency in our discussion when relating one result to another. We remark that there are relatively small quantitative changes to our results when using different initializations of M and Win; however, the main characteristics of our results remain similar. The same is said for Figs. 7 and 8, but the dimension of Win is decreased in order to take into consideration the lower-dimensional training data.\nDATA AVAILABILITY\nThe data that support the findings of this study are available from the corresponding author upon reasonable request.\nREFERENCES\n1D. Verstraeten, B. Schrauwen, M. d’Haene, and D. Stroobandt, Neural Netw. 20, 391 (2007). 2H. Jaeger, German National Research Center for Information Technology GMD Technical Report, Bonn, Germany, 2001, p. 148. 3W. Maass, T. Natschläger, and H. Markram, Neural Comput. 14, 2531 (2002). 4L. Grigoryeva and J.-P. Ortega, Neural Netw. 108, 495 (2018). 5T. L. Carroll, Chaos 30, 121109 (2020). 6A. G. Hart, J. L. Hook, and J. H. Dawes, Physica D 421, 132882 (2021). 7A. Hart, J. Hook, and J. Dawes, Neural Netw. 128, 234 (2020). 8P. Verzelli, C. Alippi, and L. Livi, arXiv:2010.02860 (2020). 9Z. Lu, B. R. Hunt, and E. Ott, Chaos 28, 061104 (2018). 10H. Jaeger and H. Haas, Science 304, 78 (2004). 11J. Pathak, B. Hunt, M. Girvan, Z. Lu, and E. Ott, Phys. Rev. Lett. 120, 024102 (2018). 12A. Griffith, A. Pomerance, and D. J. Gauthier, Chaos 29, 123108 (2019). 13A. Haluszczynski and C. Räth, Chaos 29, 103143 (2019). 14Z. Lu, J. Pathak, B. Hunt, M. Girvan, R. Brockett, and E. Ott, Chaos 27, 041102 (2017). 15E. N. Lorenz, J. Atmos. Sci. 20, 130 (1963). 16A. Chattopadhyay, P. Hassanzadeh, and D. Subramanian, Nonlinear Process. Geophys. 27, 373 (2020). 17J. Herteux and C. Räth, Chaos 30, 123142 (2020). 18K. L. Briggman and W. Kristan, Jr., Annu. Rev. Neurosci. 31, 271 (2008). 19A. Flynn, V. A. Tsachouridis, and A. Amann, Chaos 31, 013125 (2021). 20K. Nakajima, Jpn. J. Appl. Phys. 59, 060501 (2020). 21G. Tanaka, T. Yamane, J. B. Héroux, R. Nakane, N. Kanazawa, S. Takeda, H. Numata, D. Nakano, and A. Hirose, Neural Netw. 115, 100 (2019). 22T. L. Carroll and L. M. Pecora, Chaos 29, 083130 (2019). 23T. Lymburn, S. D. Algar, M. Small, and T. Jüngling, Chaos 31, 033121 (2021). 24W. A. Barbosa, A. Griffith, G. E. Rowlands, L. C. Govia, G. J. Ribeill, M.-H. Nguyen, T. A. Ohki, and D. J. Gauthier, arXiv:2102.00310 (2021). 25K. L. Briggman and W. B. Kristan, J. Neurosci. 26, 10925 (2006). 26I. R. Popescu and W. N. Frost, J. Neurosci. 22, 1985 (2002). 27S. Lieske, M. Thoby-Brisson, P. Telgkamp, and J. Ramirez, Nat. Neurosci. 3, 600 (2000). 28K. Inoue, K. Nakajima, and Y. Kuniyoshi, Sci. Adv. 6, eabb3989 (2020). 29Z. Lu and D. S. Bassett, Chaos 30, 063133 (2020). 30A. Ceni, P. Ashwin, and L. Livi, Cognit. Comput. 12, 330 (2020).\n\nChaos 31, 073122 (2021); doi: 10.1063/5.0055699 © Author(s) 2021\n\n31, 073122-11\n\n"}