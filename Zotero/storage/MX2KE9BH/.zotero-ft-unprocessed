{"indexedPages":15,"totalPages":15,"version":"24","text":"Multifunctionality in a reservoir computer\nCite as: Chaos 31, 013125 (2021); https://doi.org/10.1063/5.0019974 Submitted: 26 June 2020 • Accepted: 18 December 2020 • Published Online: 12 January 2021\nAndrew Flynn, Vassilios A. Tsachouridis and Andreas Amann\nARTICLES YOU MAY BE INTERESTED IN On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD Chaos: An Interdisciplinary Journal of Nonlinear Science 31, 013108 (2021); https:// doi.org/10.1063/5.0024890 Transfer learning of chaotic systems Chaos: An Interdisciplinary Journal of Nonlinear Science 31, 011104 (2021); https:// doi.org/10.1063/5.0033870 Attractor reconstruction by machine learning Chaos: An Interdisciplinary Journal of Nonlinear Science 28, 061104 (2018); https:// doi.org/10.1063/1.5039508\n\nChaos 31, 013125 (2021); https://doi.org/10.1063/5.0019974 © 2021 Author(s).\n\n31, 013125\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nMultifunctionality in a reservoir computer\n\nCite as: Chaos 31, 013125 (2021); doi: 10.1063/5.0019974 Submitted: 26 June 2020 · Accepted: 18 December 2020 · Published Online: 12 January 2021\nAndrew Flynn,1,a) Vassilios A. Tsachouridis,2 and Andreas Amann1\nAFFILIATIONS 1School of Mathematical Sciences, University College Cork, Cork T12 XF62, Ireland 2Raytheon Technologies Research Center Ireland, Cork T23 XN53, Ireland\na)Author to whom correspondence should be addressed: andrew_ﬂynn@umail.ucc.ie\n\nView Online\n\nExport Citation\n\nCrossMark\n\nABSTRACT\nMultifunctionality is a well observed phenomenological feature of biological neural networks and considered to be of fundamental importance to the survival of certain species over time. These multifunctional neural networks are capable of performing more than one task without changing any network connections. In this paper, we investigate how this neurological idiosyncrasy can be achieved in an artificial setting with a modern machine learning paradigm known as “reservoir computing.” A training technique is designed to enable a reservoir computer to perform tasks of a multifunctional nature. We explore the critical effects that changes in certain parameters can have on the reservoir computers’ ability to express multifunctionality. We also expose the existence of several “untrained attractors”; attractors that dwell within the prediction state space of the reservoir computer were not part of the training. We conduct a bifurcation analysis of these untrained attractors and discuss the implications of our results.\n© 2021 Author(s). All article content, except where otherwise noted, is licensed under a Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/). https://doi.org/10.1063/5.0019974\n\nAdvancements in machine learning often arise from a “two-way street” between neuroscientific observation and mathematical representation. In this paper, we stroll through such a street with inspiration from “multifunctional neural networks.” These are networks of neurons whose activity patterns can change on the demand of performing a given duty, but synapses remain fixed. We conceptualize multifunctionality in the context of dynamical systems and machine learning by using a reservoir computer as a means to realize this neurological feat in an artificial setting. More specifically, we train a Reservoir Computer to imitate the dynamics of numerous chaotic attractors from different sources based on a given initial condition. To do this, we design a training technique that “blends” and weights data from these chaotic attractors. We explore how different weightings and changes in the memory of this artificial neural network effect the desired learning outcomes. In doing so, we uncover some “behind-thescenes” bifurcations of several other attractors found to be lurking within the prediction state space that interfere with the network capacity to express multifunctionality. Above all, this paper identifies some new application areas suitable to a reservoir computer and broadens the current understanding of the dynamical capabilities inherent to this learning system.\nI. INTRODUCTION\nMultifunctionality is an essential element of biological neural networks.1–3 These multifunctional networks are distinct pools of\n\nneurons capable of performing a multitude of mutually exclusive tasks. To elaborate with an example, it was found that a subset of the same bundle of neurons in the brain of the medicinal leech (Hirudo medicinalis) can switch their activity pattern once it senses a change in its surroundings to drive either a swimming or crawling motion.4 It is reported that a cluster of neurons located in the pre-Bötzinger complex (a region of the mammalian brain stem) is responsible for regulating a switching between different respiratory patterns.5 Depending on a particular input, the neurons in this region of the brain can alter their activity pattern accordingly to elicit a switching between eupneic (regular) breathing, sighing, or gasping. Furthermore, it is argued that multifunctionality in neural networks may naturally emerge from an efficient use of limited resources (in this case, neurons) and thus an evolutionary advantage in enduring environmental changes, reflecting the developmental history of certain organisms.6\nNevertheless, what is ubiquitous among these multifunctional neural networks is that they in principle resemble a system with more than one modus operandi. Based on a particular input, there is a distinct activity pattern expressed by the neurons in the network in order to perform one of the many tasks required of it. When needed, these multifunctional neurons switch to another activity pattern to collectively execute a different task while the network connections remain fixed. Therefore, if an artificial neural network was trained to sustain more than one desired activity pattern, it would in this sense be multifunctional. From a dynamical system perspective, this\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-1\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\ntype of behavior is akin to a multistable system or a system with a coexistence of attractors. For further reading on multistable systems, see Pisarchik and Feudel.7\nThere is much to be gained in the pursuit of artificial intelligence by articulating our current knowledge of biological neural networks and dynamical systems in machine learning environments. In this paper, we employ such a bilateral rationale to encapsulate multifunctionality in an artificial neural network by training a “Reservoir Computer”8–10 (RC) to facilitate the coexistence of more than one desired activity pattern.\nThe RC approach to machine learning has been successfully applied to a number of problems, for example, time series prediction,11 visual identification tasks,12 real-time detection of epileptic seizures,13 inferring from limited time series data, unmeasured state variables,14 Lyapunov exponents,15 and causal dependencies between variables.16\nThe basis of our research involves using a recent formulation of a continuous-time RC, presented by Lu et al.17 Here, the RC was trained to perform short term predictions of a chaotic Lorenz system18 and reconstruct the “climate” (qualitatively similar dynamical behavior) of its famous butterfly shaped chaotic attractor. Taking this result into account, we train a RC to promote a coexistence of reconstructed chaotic attractors in its prediction state space, thus becoming multifunctional. In order to demonstrate the flexibility of our approach, we consider training scenarios in which the climate of these chaotic attractors is reconstructed from a variety of sources. For example, we consider the case where these chaotic attractors are generated from two different systems entirely. We devise a training technique to “blend” data with a certain weighting parameter from these chaotic attractors. The choice of this weighting along with a parameter involved in tuning the memory of the RC is critical to achieving multifunctionality. We investigate the optimal setting of these parameters, from which we infer the regions in this parameter space where the RC achieves multifunctionality.\nHowever, while we train the RC to realize more than one chaotic attractor in its prediction state space, we find several “untrained attractors” also residing here. These attractors inhabit the prediction state space but were not part of the training and limit the regions in which multifunctionality is obtained. A bifurcation analysis of these untrained attractors reveals some interesting dynamics where, for example, one of these attractors undergoes a period-doubling route to chaos.\nThe structure of the rest of the paper is as follows. In Sec. II, we provide details of the RC approach to attractor reconstruction and present the training procedure we use to achieve multifunctionality in a RC. Next, in Sec. III, the problem of epitomizing multifunctionality in a RC is further conceptualized. The trajectories on the chaotic attractors we use as our training data are also given here. In Sec. IV, we present our main findings and then provide an extended discussion of our results in Sec. V.\nII. RESERVOIR COMPUTING\nEcho-State Networks8 (ESNs) and Liquid-State Machines9 (LSMs) are two independently proposed designs of artificial neural networks with recurrent connections that can be trained to provide a self-sustained activity pattern. While ESNs were engineered\n\nin the context of machine learning and LSMs were developed from a computational neuroscience perspective, they share a similar philosophy and as a result have become increasingly synonymous under the umbrella term of “Reservoir Computing” or indeed a “Reservoir Computer” (RC).10 Both are based upon the notion that as long as the internal connections, or in this case the “reservoir,” possess certain characteristics, it is not necessary to adapt the internal weights of the network in order to achieve a desired learning outcome. Instead, it is sufficient to find an appropriate readout layer for a given task. Consequently, as the internal connections of the reservoir remain unaltered throughout the training, it can be represented physically. There are many interesting constructions of these “Physical Reservoir Computers,” using, for example, an optoelectronic system19 or an octopus inspired soft robotic arm.20 For a recent review, see Tanaka et al.21\nFor the purpose of keeping this paper self-contained, we now outline the anatomy and implementation stages of the RC setup we use that was proposed by Lu et al.17 We also present the technique we devised in order to train the RC to reconstruct the climate of more than one chaotic attractor based on a given initial condition.\n\nA. Listening stage\nIn the listening stage, as illustrated in Fig. 1, the input data are used to drive the “listening reservoir” away from its initial state. This drive-response system evolves according to\n\nr˙(t) = γ −r(t) + tanh (M r(t) + σ Win u(t)) .\n\n(1)\n\nHere, r(t) ∈ RN is the state of the reservoir at a given time t and\nN denotes the number of artificial neurons. γ is a parameter aris-\ning from the transformation of the discrete-time formulation in Jaeger and Haas11 to continuous-time. M ∈ RN×N is the adjacency\nmatrix representing the internal network connections of the reservoir. The input strength parameter, σ , and Win ∈ RN×D, the input matrix, when multiplied together represent the weight given to the input vector u(t) ∈ RD as it is projected into the reservoir. Here, D is\nthe dimension of the input data. We compute trajectories of Eq. (1) using the 4th order Runge–Kutta method with time step τ = 0.01.\nIn the listening stage, we allow Eq. (1) to evolve from t = 0 to t = tlisten = 200 in order to remove any dependency the RC may have on its initial condition and synchronize to the input.\n\nB. Training stage\nIn the training stage, we focus on the data generated from Eq. (1) and u(t) for tlisten ≤ t ≤ ttrain = 400. The aim of training is to determine a post-processing function of the reservoir state, ψˆ (r(t)), which can replace the input. Like in Lu et al.,17 we consider post-processing functions of the following form:\n\nψˆ (r(t)) = Wout q(r(t)).\n\n(2)\n\nHere, q(r(t)) ∈ R2N is a vector function that returns a vector where the first N elements are r(t) and the second N elements are r2(t). While this step differs from many other manifestations of the readout layer, it is said to be advantageous over linear functions of r(t) and embeds further nonlinearity in the network. The “output\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-2\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 1. Listening reservoir: The input signal u(t) is projected by σ Win to drive a response from the reservoir state, r(t).\n\nmatrix,” Wout ∈ RD×2N, is determined by a ridge regression procedure which we now outline.\nEach evaluation of q(r(t)) during the training time is stored in columns of the regularization matrix, X,\n\nX = q(r(tlisten)) q(r(tlisten + τ )) · · · q(r(ttrain)) . (3)\n\nThe target data matrix, Y, is constructed in a similar manner,\n\nY = u(tlisten) u(tlisten + τ ) · · · u(ttrain) .\n\n(4)\n\nFinally, Wout is calculated as\n\nWout = YXT XXT + β I −1 ;\n\n(5)\n\nhere, β is the regularization parameter, and its role is to discourage overfitting and penalize large elements of Wout from occurring. I is the identity matrix of the appropriate dimension.\n\nC. Predicting stage\nIn the predicting stage, we compute solutions of the “predicting reservoir” described by the following equation:\nr˙ˆ(t) = γ −rˆ(t) + tanh M rˆ(t) + σ WinWout q(rˆ(t)) , (6)\nwith rˆ(0) = r(ttrain). This setup is illustrated in Fig. 2. If the training was successful, the readout from the reservoir,\nWout q(rˆ(t)), should be an approximation of the original input which we denote as uˆ (t) ≈ u(t).\nIn our numerical experiments, we set N = 1000 and the reservoir state is initialized at the beginning of the listening stage as r (0) = (0, 0, . . . , 0)T = 0T for all simulations. Here, T denotes the transpose operation. M is constructed as a random matrix of sparse Erdös–Renyi connectivity with a specific spectral radius, ρ. To elaborate, the matrix is designed such that each element is chosen independently to be nonzero with probability P = 0.04 (i.e., sparsity = 0.04 or degree = 40), and these nonzero elements are chosen\n\nFIG. 2. Predicting reservoir: The readout layer Wout q(rˆ(t)) replaces the external input to the reservoir.\nuniformly from (−1, 1). This random sparse matrix is rescaled such that the magnitude of its largest eigenvalue is ρ. For example, if ρ is close to 1, this in effect means that the input takes a long time to die out within the reservoir, which is more preferable in tasks requiring a large memory.22 The Win matrix is designed such that each row has only one nonzero randomly assigned element, chosen uniformly from (−1, 1). The time damping factor is kept constant at γ = 10, while ρ, σ , and β are varied in order to achieve a desired learning outcome.\nWe remark that it is difficult to choose ρ, σ , γ , and β for a specific task. To combat this, gradient23 and Bayesian24 based parameter optimization algorithms have been developed to reduce the error in time series prediction problems. Such methods go beyond the scope of the current paper but may be useful in further studies. Instead, it is our focus to understand the mechanisms that give rise to multifunctionality in a RC. We do this by exploring the dynamics exhibited by the RC in a range of parameter values. As we will see, a multifunctional RC requires that for a given set of parameters, it can reconstruct the climate of more than one attractor. However, certain attractors may require different parameter settings to be reconstructed. In this paper, we investigate the effects in performance that changing ρ has in terms of the RC reconstructing a pair of attractors and, therefore, achieving multifunctionality. We choose to work with a random Erdös–Renyi topology in order to provide the RC with enough dynamical flexibility to solicit multistable dynamics.\nNext, we present the training technique we designed that combines data from different sources in order to construct a single output matrix that allows for the reconstruction of more than one chaotic attractor.\nD. Training with the “blending technique”\nWe adapt the regression procedure from Sec. II B to instead use data from two input sources and the corresponding reservoir output in both training stages. From a philosophical perspective, it is necessary that the matrices M and Win and parameters ρ, σ , and β remain identical when generating both training data sets. These\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-3\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 3. Illustration of a multifunctional RC.\n\nby Wolpert and Kawato26 focuses on a “modular neural network” setup whereby a given trained network is singled out among others and brought into operation by a responsibility signal in order to generate the specific behavior required from it. Furthermore, it was demonstrated by Tani et al.27 that multiple temporal patterns can be learned using the recurrent neural network with parametric biases setup. Here, the parametric biases act as bifurcation parameters that change the dynamical regime of the network in order to generate a specific behavior. In contrast, our approach is not modular or requires a conscious change in parameters in order to reconstruct different attractors using the same network. When the training is successful, the multifunctional RC setup presented in this paper operates on a global scale where the artificial neurons are specifically organized in the network to perform more than one task based on a given IC.\n\ndata are stitched together in what we call the “blending technique” with the “blending” or “weighting” parameter, α ∈ [0, 1]. The particular construction of the regularization matrix, X, and the target data matrix, Y, used in the regression are now outlined.\nFirst, the training data collected from the reservoir regarding each individual attractor, XS1 and XS2 , for S1 and S2 some arbitrary attractors, are grouped together and “blended” in the following concatenation:\n\nXC = αXS1 , (1 − α) XS2 .\n\n(7)\n\nBased on this construction, when α = 0 or α = 1, one data set completely dominates the training. The same procedure is applied to the corresponding target data matrices in order to obtain the equivalent YC.\nTo avoid any biases in the concatenation step, we take advantage of the memory-less based readout layer by randomly reordering each column of the matrices, XC and YC, corresponding to the input and reservoir state at a given time. These matrices are used in the ridge regression formula in Eq. (5).\nThe aim is to find an α that will give rise to an output matrix, Wαout, which, depending on the initial condition (IC), allows the RC to reconstruct one or the other attractor. We provide a schematic of the desired outcome in Fig. 3.\nWe remark that recently, a RC was used in a chaotic source separation problem where it was trained using a different method of blending signals from various chaotic sources.25 In contrast to our method, Krishnagopal et al.25 consider training a RC to separate a blended input formed by a linear combination of two differently weighted chaotic signals into its constituents. Once the RC is trained on this sum of mixed signals, the aim is to suppress one of the signals and continue to predict the evolution of the other chaotic time series. This work differs from the results of the current paper as we aim to train a single RC on a weighted and randomly blended training data set from two different chaotic attractors and predict the evolution of either chaotic attractor based on a given IC.\nRelated to the concept of multifunctionality in artificial neural networks is the notion of “systems within systems.” In this approach, distinct sub-networks are assigned particular duties within a larger network to collectively perform different behaviors. For example, the modular selection and identification control technique proposed\n\nIII. MULTIFUNCTIONALITY AND DYNAMICAL SYSTEMS\nThe characterization of neuronal behavior regularly invokes the language of dynamical systems theory.6,28 Conceptualizing certain traits of neurons from this perspective can act as a bridge between biological and artificial neural networks where advancements in one can contribute to the other.\nConsidering the claim made in Sec. I that a multifunctional neural network in principle resembles a system with a coexistence of attractors and that a RC can be trained to reconstruct the climate of a chaotic attractor, we pose the following: can a RC be trained to permit a coexistence of reconstructed attractors? Such a RC can be said to, in the spirit of our claim, express multifunctionality.\nTo demonstrate this, we consider the following tasks that require multifunctionality by training a RC to reconstruct a coexistence of chaotic attractors from\n\n• Case I: a multistable system. • Case II: a system with different parameter settings. • Case III: two different systems entirely.\n\nWe remark that such tasks closely resemble the chaotic and highly variable behavior expressed by certain multifunctional neural networks as observed in nature.29,30 Furthermore, coexisting chaotic attractors are also found to occur in some low-dimensional models of neuronal systems.31,32\nIn order to provide an adequate testing ground, we require input data from a system that allows for the generation of multiple coexisting chaotic attractors under a wide range of parameters. An example of such a system was presented in Guan et al.33 and described by the following set of equations:\n\nx˙1(t) = a x1(t) − x2(t) x3(t) − x2(t) + d,\n\nx˙2(t) = −b x2(t) + x1(t) x3(t),\n\n(8)\n\nx˙3(t) = −c x3(t) + x1(t) x2(t).\n\nHere, x(t) = (x1(t), x2(t), x3(t))T defines the state of the system at a given time t. a, b, c, and d are the system parameters.\nFor example, when setting a, b, c, d = (5, 15, 3, 12), there is\na coexistence of two single-scroll chaotic attractors as illustrated in Fig. 4. Initializing Eq. (8) with xA1 (0) = (1, 1, 1)T results in the\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-4\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nmultifunctionality, we suggest studying the effect of decreasing the distance between two disjoint attractors.\n\nFIG. 4. Coexisting attractors in the state space of Eq. (8) for (a, b, c, d) = (5, 15, 3, 12) and xA1,A2 (0) = (1, 1, ±1)T .\nstate of the system settling on the attractor A1, seen as the blue trajectory in Fig. 4. The other attractor, A2, indicated by the orange trajectory in Fig. 4 is arrived at once Eq. (8) is initialized from xA2 (0) = (1, 1, −1)T.\nTrajectories on these attractors, A1 and A2, are considered the training data for Case I.\nIn Case II, we set the problem of reconstructing a double-scroll chaotic attractor in addition to the attractor A2 from Fig. 4. This particular double-scroll chaotic attractor, which we call B1, is reached by initializing Eq. (8) with xB1 (0) = (1, 1, 1)T and setting the system parameters as a, b, c, d = (5, 8, 2, 2) as shown in Fig. 5.\nIn Case III, we consider reconstructing A2 and the chaotic butterfly attractor, L, generated by the Lorenz system.18\nWe remark that the Lorenz attractor L and the attractors chosen from Eq. (8) share no common region in the state space. Otherwise, this adds a further level of difficulty in training a RC to distinguish which attractor a given trajectory belongs to. Alternatively, to expose further criteria needed for the RC to exhibit\n\nIV. RESULTS\nIn this section, we illustrate the events in which a RC was trained to exhibit multifunctionality by successfully reconstructing the coexistence of attractors as specified in Cases I–III. We focus on establishing the optimal blending of the training data with respect to ρ so as to determine the regions in which multifunctionality was achieved. Following these observations, we examine the circumstances in which the reconstruction of a given attractor fails and detect a number of “untrained attractors”; attractors residing within the prediction state space were not part of the training. We then track the evolution of these attractors with respect to α and ρ and uncover a number of “behind-the-scenes” bifurcations.\n\nA. Attractor reconstruction\nThere are many means of assessing the accuracy of a predicted time series. In this work, we choose to calculate θS (WSout) as the Normalized Root Mean Square Error (NRMSE) of the prediction in comparison with the target time series averaged over all state variables of a given attractor S. θS (WSout)i for the ith state variable is calculated as\n\nθS (WSout)i =\n\n1 tpredict −t∗\n\ntpredict t=tpredict −t∗\n\nui(t) − uˆ i(t) 2\n\n.\n\n|max(ui(t)) − min(ui(t))|\n\n(9)\n\nIn our results, we set tpredict = 600 as the prediction end time and tpredict − t∗ is the time that error sampling begins from. ui(t) and uˆ i(t) are the ith state variables of the target and predicted time series.\nThe max (·) and min (·) functions are measures of the maximum and minimum value of the time series evaluated from tpredict − t∗ to tpredict. The closer θS (WSout) is to 0, the more accurate the prediction of S. It was found empirically that for θS (WSout) > δ = 0.35, attractor reconstruction fails. Throughout our work, we keep β = 10−2,\nσ = 0.2 in Cases I and III and let σ = 0.4 in Case II.\n\nFIG. 5. The attractor, B1, arrived at by computing solutions of the system [Eq. (8)] initialized from xB1 (0) = (1, 1, 1)T with parameter values set to (a, b, c, d) =\n(5, 8, 2, 2).\n\nFIG. 6. θS (WSout) vs ρ when using the task speciﬁc matrices to reconstruct the attractor S from Cases I–III.\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-5\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 7. Case I (ρ = 0.7): (a) εA1 and εA2 vs α. (b) and (c) Attractor reconstruction using WA out1 and WA out2 and Wαout for α = 0.5.\n\nFirst, we conduct an error analysis of the predicted time series when using the task specific matrices, WAou1t , WAou2t , WBou1t, and WLout, to determine if the attractors can be reconstructed in the usual sense. In Fig. 6, we provide a picture, where having set t∗ = ttrain, the error\nanalysis indicates that attractor reconstruction was achieved for all ρ ∈ [0.1, 1.1] using each of the task specific matrices as θS (WSout) < δ in Cases I–III.\nWith this established, we now search for values of α that give rise to a single readout matrix, Wαout, which allows the RC to reconstruct either of the attractors specified in Cases I–III. After applying\nthe blending technique, we calculate the following error function of\nthe particular readout matrix used to reconstruct a given attractor:\n\nεS (α) = θS Wαout /θS WSout .\n\n(10)\n\nHere, θS Wαout is the NRMSE when using the blending technique to reconstruct an attractor S with a certain α. This measure of\nerror implies that if εS (α) < 1, the prediction of S was more accurate when using Wαout over WSout with the opposite being said if εS (α) > 1.\nStarting with the task of reconstructing attractors from a mul-\ntistable system in Case I, we set ρ = 0.7 with the resulting εS (α) vs\nα plot shown in Fig. 7(a).\n\nHere, we see that on one hand as α is increased from 0, there\nis a large decrease in εA1 and stays relatively close to 1 for α 0.22, while on the other hand, εA2 remains relatively near 1 for α 0.88 and then grows as α is increased thereafter. From this, we deduce\nthat for 0.22 α 0.88, multifunctionality is expressed by the RC.\nWe illustrate in Figs. 7(b) and 7(c) a successful implementation of the blending technique for α = 0.5. The predicted trajectory on A1 and A2 when using the task specific matrices, WAou1t and WAou2t , is plotted in orange, and the predictions using the multifunctional output matrix, Wαout, are plotted in green. A trajectory on the actual attractors from Eq. (8) in each figure is plotted in blue.\nWe conduct a similar analysis for Cases II and III with Fig. 8\nillustrating examples where the RC was successfully trained to be\nmultifunctional. We see in Fig. 8(a) that when training the RC to\nreconstruct both B1 and A2 in Case II, the desired coexistence of chaotic attractors is achieved for ρ = 0.3 and α = 0.5. We find that in Case III, when setting ρ = 0.85, the RC is successfully trained to\nexpress multifunctionality as it can reconstruct both L and A2 for α = 0.65 as seen in Fig. 8(b).\nThe results illustrated in Figs. 7 and 8 show that a RC can\nbe trained to exhibit multifunctionality. Furthermore, this broad-\nens the current set of applications a RC is capable of. We show that\ninstead of having to change parameters in a system for it to exhibit a\n\nFIG. 8. Illustration of attractor reconstruction in (a) Case II for (α, ρ) = (0.5, 0.3) and (b) Case III for (α, ρ) = (0.65, 0.85).\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-6\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 9. (a) and (b) Long-term behavior of prediction in the (α, ρ)-plane for Case I. Each color characterizes the attractor the RC eventually settles to starting from a particular IC: Initial Condition. (c) Plotted in blue are the regions of multifunctionality.\n\ndifferent behavior, one can merge separate modes of operation from various parameter choices of the system to coexist in the prediction state space of a multifunctional RC. We also demonstrate that the combination of attractors is not limited to a single system as it is possible to combine attractors from different systems to coexist. We remark that given this ability, there is the prospect of designing an appropriate controller for Eq. (6) to switch between attractors. For further reading on “inter-attractor control,” see Richter.34\nAlthough we have illustrated instances in which appropriate values of α were found to give rise to multifunctionality, this cannot be said for all α values or for other choices of γ , ρ, σ , and β. Moreover, relying on an error analysis alone is not sufficient enough to classify the parameter regions in which multifunctionality is achieved.\nThis RC is designed such that if attractor reconstruction fails, then the predicted trajectory will not blow up to infinity in a finite amount of time. However, the prediction can decay toward some other stable attractor. Furthermore, given the nature of our study, it is also possible that the RCs predicted trajectory on one chaotic attractor can switch to the other chaotic attractor.\nFollowing this argument, in Sec. IV B, we employ a means of characterizing the resultant attractor that the prediction settles to and from this identify the regions in the (α, ρ)-plane where multifunctionality is achieved.\nB. Exploring multifunctionality in the (α, ρ)-plane\nIn this section, we analyze the long-term behavior of the RC in the prediction stage [Eq. (6)] initialized with rˆ(0) corresponding to A1, A2, B1, or L (for the appropriate case) and trained for a given α ∈ [0, 1] and ρ ∈ [0.1, 1.1].\nWe choose to assign a color to each point in the (α, ρ)plane that characterizes the prediction of a given attractor. More specifically, a point in the (α, ρ)-plane is colored:\n1. Yellow—if the predicted time series is periodic for tpredict − 40 ≤ t ≤ tpredict, we say that the prediction has decayed to some limit cycle.\n2. Green—if the predicted time series remains constant for tpredict − 10 ≤ t ≤ tpredict, we say that the prediction has decayed to some fixed point.\n\n3. Purple—if for t∗ = 40, θS Wαout ≤ δ for the predicted time series in comparison with the target time series of the other chaotic attractor, we then say that the prediction has switched from one chaotic attractor to the other.\n4. Blue—if conditions 1–3 are not fulfilled and if for t∗ = 40 that θS Wαout ≤ δ for a given attractor S, we then say that the climate of S was well reconstructed.\n5. Red—if otherwise to signal that closer inspection of the prediction is needed.\nThe result of this analysis for Case I is shown in Fig. 9. As expected, we see that attractor reconstruction is impossible when initializing the RC in a prediction mode with rˆ(0) corresponding to A1 in Fig. 9(a) [A2 in Fig. 9(b)] for α = 0 (1). We now know that the prediction in this scenario consistently decays toward some fixed point for all values of ρ. However, when increasing (decreasing) α from 0 (1), there is a more varied sequence of events for a given ρ. Prior to achieving attractor reconstruction, the predicted trajectory on A1 (A2) at times tends toward some limit cycle or switches to A2 (A1). Figure 9(a) also shows that for large ρ, the RC favors the reconstruction of A2 as opposed to A1 where the prediction of A1 mainly switches to A2 until some critical α value where attractor reconstruction is achieved. Nevertheless, there is some middle ground where both attractors can be successfully reconstructed for a given pair of α and ρ, and this common blue area between Figs. 9(a)–9(b) is the regions in which we say multifunctionality was achieved. We provide a separate picture in Fig. 9(c) to explicitly show these regions. Through the same reasoning, we show in Figs. 10(a) and 10(b) the regions in the (α, ρ)-plane in which the RC was successfully trained to express multifunctional behavior for both Cases II and III. We see a relatively large common blue region in the (α, ρ)plane for Case I in Fig. 9(c). However, for Cases II and III, the regions of multifunctionality in Figs. 10(a) and 10(b) are relatively much smaller. Cases II and III require a higher level of dynamical flexibility from the RC as not only do the chaotic attractors come from different settings of Eq. (8) and different systems entirely but also vary characteristically, i.e., single-scroll and double-scroll. A particular setup of the RC may happen to favor reconstructing one\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-7\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 10. Regions of multifunctionality in the (α, ρ)-plane for (a) Case II and (b) Case III.\n\nflavor of attractor over the other, thus a contributing factor toward the reduced regions of multifunctionality seen here.\nWhile in each of the explored cases, the target chaotic attractors are separated in the state space, they do share some dynamical similarities that may be just as vital in order to achieve multifunctionality. For example, the chosen attractors in all three of the studied cases evolve along a similar timescale. We now consider the pair of chaotic attractors in Case I and investigate the relationship between the RC capacity to exhibit multifunctionality when changing the timescale of A1 while keeping the timescale of A2 fixed. To do this, we introduce a new parameter, A1 , which is used to control the timescale of A1. We multiply the RHS of Eq. (8) by A1 and initialize the system with xA1 (0) = (1, 1, 1)T to generate solutions of A1 with a modified timescale. If A1 < 1, the dynamics are slowed down, and if A1 > 1, the dynamics are sped up.\nWe now investigate and identify the regions in the ( A1 , ρ)plane where multifunctionality is achieved. We do this by employing the previous method of characterizing the long-term behavior of the RC prediction of either A1 or A2 when using Wαout with α = 0.5 and take the common regions where the RC can successfully reconstruct either attractor depending on the IC. The result of this is shown in Fig. 11.\nHere, in Fig. 11, we see that if A1 evolves along a relatively longer or shorter timescale in comparison with A2, then there is a point where multifunctionality is lost for all ρ values. This reveals the limits upon which multifunctionality can be achieved in this scenario by exposing the RC dynamical capacity to facilitate the coexistence of increasingly dissimilar chaotic attractors in its prediction state space.\nNow, if we focus on the prediction of A1 for ρ = 0.7 as α is increased from 0 in Fig. 9(a), we see that before multifunctionality is achieved, the prediction repeatedly falls to a fixed point. We plot these fixed points for various values of α in Fig. 12. Also seen here is a case where the prediction switches from A1 to A2 when α = 0.2.\nWe ask, what happens to these fixed points in Fig. 12 as multifunctionality is achieved? Is it the case that the successful\n\nreconstruction of A1 entirely takes the place of these fixed points in the prediction state space or do these fixed points still exist and we are unable to see them? Furthermore, are there other attractors lurking in this prediction state space that we are not immediately aware of? We delve further into these questions in Sec. IV C and highlight the important role of ICs in the prediction stage of the RC. C. Detecting untrained attractors\nTo get a broader picture of the prediction state space for a given ρ and α in Case I, we initialize the RC in the prediction mode with many (1000) random initial conditions (RICs) and observe the resultant trajectories.\nFIG. 11. Regions of multifunctionality in the ( A1 , ρ)-plane when changing the timescale of A1 in Case I with timescale parameter A1 .\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-8\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 12. Case I (ρ = 0.7, IC: A1): x3(t) vs t for A1 colored in black and the predicted trajectories for various values of α as indicated in the plot legend.\n\nIn Fig. 13, we set ρ = 0.7 and train the RC for α = 0.45, 0.50, and 0.55. We plot xξ3i as the trajectories of the x3 variable as predicted by the RC starting from the ith RIC.\nFor α = 0.5, we see that there exists a stable fixed point located within the middle of both reconstructed chaotic attractors. As we begin to apply more weight to the data from A2 with α = 0.45, we see another stable fixed point appearing closer to the reconstructed A2. Similarly, for α = 0.55, we find a stable fixed point\nFIG. 13. Time trace of the predicted x3 variable, with the RC trained for a given α and ρ as indicated above each plot, starting from the random initial condition ξi for i = 1, 2, . . ..\n\nFIG. 14. The evolution of four randomly chosen elements of the Wαout matrix with respect to changes in α and ρ.\ncloser to the reconstructed A1. Therefore, while the RC was successfully trained to reconstruct the climate of both A1 and A2, there are additional attractors populating the prediction state space that were not involved in the training, and we call these the “untrained attractors.” The results in Fig. 13 show that small changes in α can give rise to a bistability of untrained attractors. This suggests that there are some “behind-the-scenes” bifurcations taking place in the prediction state space. However, further discussion is needed to consider α as a bifurcation parameter of the RC.\nWhile ρ is a parameter of the RC itself, α is a parameter that is strictly involved in the training procedure. Therefore, to consider α as a bifurcation parameter of the RC, we need to assess if for a small change in α, there is a relatively smooth change in the elements of the Wαout matrix generated from a specific combination of α and ρ. In other words, a continuous function that maps (α, ρ) → Wαout needs to exist.\nTo expand upon this notion, we consider some randomly chosen elements of Wαout and observe their evolution with respect to α and ρ. The result of this is shown in Fig. 14 for four randomly chosen elements of Wαout.\nThe smoothness of these surface plots indicates that a relatively small change in α or ρ in turn gives rise to a small change in the elements of Wαout to which we generalize contributes to an overall change in the dynamics of the RC. From this, we now consider α as a bifurcation parameter of the RC. We can view our training procedure as a smooth map from the hyper-parameters ρ\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-9\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 15. Evolution of untrained attractors (stable branches of ﬁxed points) in (α, x3)-plane for (a) ρ = 0.7 and (b) ρ = 0.77.\n\nand α to a matrix Wαout in the RD×2N space. Therefore, by considering these hyper-parameters, ρ and α, this mapping generates a two-dimensional structure in the RD×2N space.\nOur study now moves toward tracking the evolution of these untrained attractors and identifying some of these “behind-thescenes” bifurcations.\nD. Bifurcation analysis of untrained attractors\nIn this section, we track the evolution of these untrained attractors first in the (α, x3)-plane for a given ρ and then in the (α, ρ)-plane.\nWe do this by initializing the state of the RC, trained for a certain α and ρ, with one of the corresponding fixed points shown in Fig. 13. We track the evolution of this fixed point with respect to α by repeating the process of incrementally changing α, retraining and initializing the state of the RC with the fixed point corresponding to the previous α. The result of this for ρ = 0.7 is shown in Fig. 15(a).\nHere, we see the extent of the bistabilities found in Fig. 13. We also find branches of fixed points and bistabilities closer to the end points of α. The evolution with respect to α of the previously found fixed point located in the middle of the chaotic attractors is now labeled as the branch FP1, and the evolution of the fixed points closer to A2 and A1 is labeled as FP2 and FP3, respectively. We label the evolution of the fixed points closest to α = 0 as the branch FP4 and those closest to α = 1 as FP5.\nGiving rise to these bistabilities are the hysteresis cycles created here. As indicated by the arrows in Fig. 15(a), when moving along the FP2 branch, as α is increased, there is a point where the state of the RC jumps to the FP1 branch. After this transition, if α was instead decreased, we then remain and continue to track along the FP1 branch to the point where the state of the RC returns to the FP2 branch and so on.\nFigure 15(a) also demonstrates that when the RC fails to reconstruct A1, then the predicted trajectory falls onto the FP4 and FP2 branches. The black points plotted in Fig. 15(a) are the fixed points that the predictions of A1 decay toward in Fig. 12. As these black\n\npoints line up directly with the branches of FP4 and FP2, we conclude that as attractor reconstruction of A1 is achieved that these fixed points do not suddenly disappear, but that their existence is intrinsic to the dynamics of this RC setup. In modes of failure, these branches of fixed points provide routes to stability should the predicted trajectory fall off A1. This also gives greater insight to the behavior of the RC at the boundaries of multifunctionality. It is also important to highlight that the fixed points located at α = 0 and 1 also occur in the task specific systems. More specifically, if the RC was successfully trained on data from only A1, then within this prediction state space exists the reconstructed attractor A1 and the same fixed point, FP5, that we find in the multifunctional setup. The same can be said in relation to A2 and FP4. Furthermore, these untrained attractors have dynamics of their own and interact among themselves giving rise to these “behind-the-scenes” bifurcations.\nThese stable branches of equilibria would ordinarily be connected by branches of unstable equilibria, but given the nature of our approach, these cannot be explicitly determined. However, as the end points of these branches are seemingly being drawn together, i.e., the right and left end points of both FP4 and FP2 and likewise for FP3 and FP5, we infer that this is a signature of the existence of unstable branches of equilibria and evidence that at these end points are saddle-node (SN) bifurcations. To help strengthen this claim, we increase ρ to 0.77 and track the evolution of the fixed points as before. The result of this is shown in Fig. 15(b) where the right and left end points of FP4 and FP2 as well as FP3 and FP5 have connected together resulting in two branches of fixed points which we call FP2,4 and FP3,5. This particular behavior is indicative of two cusp bifurcations taking place about α ≈ 0.034 and 0.988 as ρ is increased from 0.7 to 0.77. In addition, we see a region of “tristability” here for 0.4867 ≤ α ≤ 0.5165 where there is an overlap between all three branches.\nWe continue exploring and characterizing the behavior of these untrained attractors by tracking their evolution in the (α, ρ)-plane. The result of this is depicted in Fig. 16.\nIn this picture, we see the previously mentioned cusp bifurcations taking place at (α, ρ) = (0.0337, 0.71) and (0.9875, 0.76).\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-10\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 16. Classiﬁcation of “untrained attractors” in the (α, ρ)-plane. FP, ﬁxed point; LC, limit cycle; PDC, period doubling cascade).\n\nWe also find a cusp bifurcation of the FP1 and FP2 branches taking place at (α, ρ) ≈ (0.48, 0.27). We denote FP1,2 as the branch of stable fixed points emerging from this cusp bifurcation.\nFigure 16 also reveals the existence of several limit cycles. As ρ is decreased from 0.27, the bistability between FP3 and FP1,2 is lost at (α, ρ) ≈ (0.5837, 0.2449). As these branches begin to drift apart, we show in Fig. 17 for ρ = 0.21 that a period-1 limit cycle, LC1, is born in this gap. Plotted here is the evolution of the maximum and minimum values of LC1 vs α. As ρ is decreased further, the bistability between the FP3 and FP1,2 branches resumes from (α, ρ) ≈ (0.577, 0.1965) resulting in the death of LC1.\nWe also see a relatively small region of tristability between FP1,2, FP7, and FP4 in Fig. 17. While the left end of FP1,2 had at one time appeared to be growing toward and potentially connecting to FP4 in another cusp bifurcation, instead it has broken off due to a cusp bifurcation taking place on FP1,2 at (α, ρ) ≈ (0.212, 0.087) resulting in a new branch of fixed points FP7 as seen in Fig. 17. The existence of FP7 is relatively brief as its own end points are quickly drawn together and disappear entirely at (α, ρ) ≈ (0.062, 0.179) in Fig. 16. This particular behavior also occurs on FP3 where we will later provide a more detailed picture and explanation of these events. We also find the beginnings of a branch of stable fixed points emerging from the right hand side of Fig. 17 which we label as FP6. Later, we will discuss some interesting properties emanating from this branch.\nGiven the abundant variety of behavior exhibited by the untrained attractors for large α and small ρ, as seen in the inset plot of Fig. 16, we provide a more expansive picture in Fig. 18 depicting how certain bifurcations arise.\n\nThe cusp bifurcation taking place at (α, ρ) = (0.965, 0.185) gives rise to a relatively small tristable region. This cusp bifurcation originates from a buckling of the FP3 branch where a new branch of stable fixed points emerges, labeled as FP8 in Fig. 18.\nA limit cycle is born at the right end point of FP3 for ρ ≈ 0.181. In Fig. 18, we plot the evolution of the maximum and minimum x3-values of this period-1 limit cycle which we label as LC2. Close to this cusp bifurcation we see for ρ = 0.18 that LC2 is contained within a Hopf-bubble as it exists between two supercritical Hopf\nFIG. 17. Tracking untrained attractors for ρ = 0.21: Max and min values of limit cycle, LC1, born at the right and left end points of the FP1,2 and FP3 branches. Also seen are the FP4, FP5, FP6, and FP7 branches of stable ﬁxed points.\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-11\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nFIG. 19. (ρ = 0.107): Evolution of FP6 as it transitions to a limit cycle and then to a chaotic attractor through an inﬁnite sequence of period-doubling bifurcations.\nAlso shown are snapshots of this limit cycle along its route to chaos.\n\nFIG. 18. Behavior of the untrained attractors in the predicted x3 direction for large α and small ρ.\nbifurcations. However, as ρ is decreased further, the amplitude of oscillation of LC2 increases and the bubble bursts. We also find that LC2 undergoes period-doubling (PD) bifurcations, the first of which was found nearby (α, ρ) = (0.959, 0.166). These PD bifurcations attribute to the significant reduction in α-values for which we are able to track LC2. Also shown in Fig. 18 are cases where LC2 subsequently encounters another PD bifurcation giving rise to a period-4 limit cycle.\nThe existence of FP8 is relatively short; as ρ is decreased, its end points are drawn closer together until this branch becomes but a single point-like attractor at (α, ρ) ≈ (0.98, 0.137) and we are no longer able to track. This event and that mentioned earlier regarding FP7 is evidence that further flavors of cusp-like bifurcations take place in the prediction state space. A codimension-3 bifurcation analysis (involving either σ , β, or γ ) could, for example, unveil a swallowtail bifurcation point (the point at which two cusp branches collide). For reading on more cusp-like bifurcations, see Kuznetsov35 or Guckenheimer and Holmes.36 Alternatively, if swallowtail bifurcation points were found to take place in lower dimensional representations of Eq. (6) (for N = 2, 3, . . .), then their existence could be generalized to the higher dimensional picture. The benefit of reducing the dimension would allow for the use of numerical continuation software such as AUTO37 in Eq. (6). Moreover, this approach facilitates the study of unstable attractors and in identifying further dynamical features inherent to Eq. (6). We leave this for future work.\nAs shown in Fig. 18, we find another branch of fixed points which we label as FP9. For ρ = 0.135, we see here that at the right end point of FP9, a limit cycle labeled as LC4 is born. Increasing ρ results in the loss of LC4 as the end points of FP9 are being drawn closer together where eventually at (α, ρ) = (0.989, 0.153), we are no longer able to track.\nThroughout Fig. 18, we plot the evolution of the previously mentioned branch, FP6. As ρ is decreased, a period-1 limit cycle, labeled as LC3, is born from the left end point of FP6. The brown region in Fig. 16 depicts the coexistence of LC3, LC4, and FP5. However, as we follow the evolution of the local maxima and minima\n\nof LC3, there are certain points in the (α, ρ)-plane where it undergoes a PD bifurcation. Furthermore, there are times at which one PD bifurcation leads to another and in turn triggers a period-doubling cascade (PDC), ultimately resulting in chaotic behavior. We find two relatively small distinct regions in Fig. 16 (colored in black and cyan) where PD bifurcations of LC3 lead to PDCs. An example of this particular sequence of PD bifurcations is shown in Fig. 19 when setting ρ = 0.107.\nStarting from α = 1, we see in Fig. 19 how FP6 evolves as α is decreased. We plot the local maxima and minima of LC3 sprouting from the left end point of FP6 for α = 0.9986 and continue to track as α is reduced further. Here, we see the first of these PD bifurcations occurring at α ≈ 0.989 where there are now two distinct local maxima and minima of LC3. This particular behavior is depicted below the bifurcation diagram in Fig. 19 with snapshots of LC3 as it travels along its route to a PDC in the (x1, x3) prediction state space. Here, we see how LC3 transitions from period-1 at α = 0.9895 to period-2 at α = 0.9885. Decreasing α further to 0.9875, we see that there has been an infinite number of PD bifurcations giving rise to a chaotic attractor. Periodic behavior briefly resumes for decreasing α further before entering another PDC as a second bout of chaos begins at α = 0.986 18 and ends at 0.986 38.\nDespite that multifunctionality is not achieved in the relatively small regions in which we find these PDCs, it is a remarkable result as it shows that when attempting to train the RC to promote a coexistence of two desired chaotic attractors, it is also possible for another chaotic attractor to exist in the background. Moreover, for a different choice of the other reservoir parameters, these PDCs could play a larger role in the prediction state space and further influence the RC ability to reconstruct a given attractor. Throughout our results, we have kept the topology of the RC matrices, M and Win fixed. Given a different initialization of these matrices, we expect that quantitative changes in these bifurcation figures would occur, but that the main characteristics would remain.\nOverall, these results indicate that regardless of the particular attractor one is attempting to reconstruct, there will always be some untrained attractor present in the prediction state space. The longterm characterization of the RC prediction of a given attractor in Figs. 9(a) and 9(b) combined with the classification of the untrained attractors in Fig. 16 provides us with a clearer picture of the prediction state space for a given α and ρ. When put together, we are able to see the regions in which multifunctionality was achieved, the\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-12\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\nmanner in which the prediction fails, and the particular untrained attractor it can tend toward. Moreover, one could argue that attractor reconstruction may fail if the basin of attraction of the desired attractor interferes with one of these untrained attractors.\nV. CONCLUSION\nIn this paper, we have demonstrated that a Reservoir Computer (RC) can be trained to exhibit multifunctionality whereby, for a given initial condition, the climate of more than one chaotic attractor can be successfully reconstructed in its prediction state space.\nIn order to train a RC to express multifunctionality, we introduce the “blending technique” as a means to combine and weight data from two different sources. We test the flexibility of this technique by training the RC to reconstruct a coexistence of chaotic attractors from a system that already exhibits multistability, two parameter settings of a system, and two different systems entirely.\nHowever, in order to achieve the desired outcome, there is a crucial dependence on certain reservoir and training parameters, in particular, the “blending” parameter, α, and the spectral radius of the reservoir’s internal connection matrix, ρ. When varying these parameters, we find an abundance of nontrivial transitions between multifunctionality and modes of failure as there is a competition between attractors in the prediction state space of the RC for a given α and ρ. For example, when attempting to reconstruct the climate of a given chaotic attractor, there are times where the prediction can switch to the other. In this case, the RC is able to reconstruct the climate of only one chaotic attractor. This behavior comes as a consequence of training a RC to reconstruct the climate of more than one attractor. Furthermore, we see that in the event of failure, the predicted trajectory on a given chaotic attractor can tend toward a limit cycle or a fixed point.\nOn closer inspection, we find that even when the RC is successfully trained to express multifunctionality, there are additional attractors existing within the prediction state space that were not involved in the training, and we call these the “untrained attractors.” Moreover, it is shown that these untrained attractors have dynamics of their own and play a significant role in the event that the desired attractor cannot be successfully reconstructed.\nBy tracking the evolution of these untrained attractors with respect to α and ρ, we have identified a number of “behind-thescenes” bifurcations. In particular, when tracking the evolution of FP6, we see in Fig. 19 how this branch of stable fixed points transitions to the limit cycle LC3, which subsequently undergoes a series of period-doubling bifurcations to the point of provoking a perioddoubling cascade inevitably leading toward the creation of a chaotic attractor. Therefore, while we try to train the RC to reconstruct a coexistence of two specific chaotic attractors in its prediction state space, there is another chaotic attractor created in a sub rosa fashion.\nMuch like our investigation of the untrained attractors in the RC prediction state space, there is also evidence of similarly occurring events in the brain, which influence and disrupt its normal behavior. It is understood that neurological disorders such as Parkinson’s disease or epilepsy occur as a result of certain active regions of the brain deviating from its normal state of operation and then becoming trapped within some undesirable behavior beyond\n\nwhich it may not be able to resume its normal function. The effort is often made to model and control these events in order to counteract the effects of these illnesses.38–40\nThe ability to combine attractors from different sources to then coexist in the same state space broadens the current set of RC applications. This demonstrates that instead of having to change parameters in a system for it to exhibit different behavior, the various desired modes of operation can coexist in the state space of a multifunctional RC where an appropriate controller could in theory be designed to switch between attractors.34 This draws further parallels to biological neural networks, where such a control mechanism is comparable to neuromodulators,41 a hierarchical system of neurons that some believe to be involved in instigating the switching of activity patterns in multifunctional neural networks. Additionally, we have illustrated that these input sources are not limited to one system. We show this in Fig. 8(b) where the RC was successfully trained to permit the coexistence of the Lorenz butterfly attractor, L, and the chaotic attractor A2 generated from Eq. (8). Naturally, the question emerges as to the amount of attractors that can be successfully trained to coexist in the RC prediction state space, and we leave this for future work.\nThe results of this paper emanate from employing the dyadic “two-way street” approach of framing neurological features in the context of dynamical systems. Moreover, this work indicates that if other hypotheses or known facets of the brain can be articulated in this manner, then there lies the potential to portray it artificially.\nACKNOWLEDGMENTS\nThis work was funded by the Irish Research Council Enterprise Partnership Scheme (Grant No. EPSPG/2017/301). A.F. would like to thank Sebastian Wieczorek for introducing him to Reservoir Computing and Paul O’Keeffe and Christopher O’Connor for their helpful conversations.\nDATA AVAILABILITY\nThe data that support the findings of this study are available from the corresponding author upon reasonable request.\nREFERENCES\n1P. A. Getting, “Emerging principles governing the operation of neural networks,” Annu. Rev. Neurosci. 12, 185–204 (1989). 2P. S. Dickinson, “Interactions among neural networks for behavior,” Curr. Opin. Neurobiol. 5, 792–798 (1995). 3E. Marder and R. L. Calabrese, “Principles of rhythmic motor pattern generation,” Physiol. Rev. 76, 687–717 (1996). 4K. L. Briggman and W. B. Kristan, “Imaging dedicated and multifunctional neural circuits generating distinct behaviors,” J. Neurosci. 26, 10925–10933 (2006). 5S. Lieske, M. Thoby-Brisson, P. Telgkamp, and J. Ramirez, “Reconfiguration of the neural network controlling multiple breathing patterns: Eupnea, sighs and gasps,” Nat. Neurosci. 3, 600 (2000). 6K. L. Briggman and W. Kristan, Jr., “Multifunctional pattern-generating circuits,” Annu. Rev. Neurosci. 31, 271–294 (2008). 7A. N. Pisarchik and U. Feudel, “Control of multistability,” Phys. Rep. 540, 167–218 (2014). 8H. Jaeger, “The ‘echo state’ approach to analysing and training recurrent neural networks—With an erratum note,” German National Research Center for Information Technology, GMD Technical Report, Vol. 148, 2001.\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-13\n\nChaos\n\nARTICLE\n\nscitation.org/journal/cha\n\n9W. Maass, T. Natschläger, and H. Markram, “Real-time computing without sta-\nble states: A new framework for neural computation based on perturbations,”\nNeural Comput. 14, 2531–2560 (2002). 10D. Verstraeten, B. Schrauwen, M. d’Haene, and D. Stroobandt, “An experi-\nmental unification of reservoir computing methods,” Neural Netw. 20, 391–403\n(2007). 11H. Jaeger and H. Haas, “Harnessing nonlinearity: Predicting chaotic systems\nand saving energy in wireless communication,” Science 304, 78–80 (2004). 12A. Jalalvand, G. Van Wallendael, and R. Van de Walle, “Real-time reservoir\ncomputing network-based systems for detection tasks on visual contents,” in\n2015 7th International Conference on Computational Intelligence, Communication\nSystems and Networks (IEEE, 2015), pp. 146–151. 13P. Buteneers, D. Verstraeten, B. Van Nieuwenhuyse, D. Stroobandt, R. Raedt,\nK. Vonck, P. Boon, and B. Schrauwen, “Real-time detection of epileptic seizures\nin animal models using reservoir computing,” Epilepsy Res. 103, 124–134 (2013). 14Z. Lu, J. Pathak, B. Hunt, M. Girvan, R. Brockett, and E. Ott, “Reservoir\nobservers: Model-free inference of unmeasured variables in chaotic systems,”\nChaos 27, 041102 (2017). 15J. Pathak, Z. Lu, B. R. Hunt, M. Girvan, and E. Ott, “Using machine learning to\nreplicate chaotic attractors and calculate Lyapunov exponents from data,” Chaos\n27, 121102 (2017). 16A. Banerjee, J. Pathak, R. Roy, J. G. Restrepo, and E. Ott, “Using machine learn-\ning to assess short term causal dependence and infer network links,” Chaos 29,\n121104 (2019). 17Z. Lu, B. R. Hunt, and E. Ott, “Attractor reconstruction by machine learning,”\nChaos 28, 061104 (2018). 18E. N. Lorenz, “Deterministic nonperiodic flow,” J. Atmos. Sci. 20, 130–141\n(1963). 19L. Larger, M. C. Soriano, D. Brunner, L. Appeltant, J. M. Gutiérrez, L. Pes-\nquera, C. R. Mirasso, and I. Fischer, “Photonic information processing beyond\nTuring: An optoelectronic implementation of reservoir computing,” Opt. Express\n20, 3241–3249 (2012). 20K. Nakajima, H. Hauser, T. Li, and R. Pfeifer, “Information processing via\nphysical soft body,” Sci. Rep. 5, 10487 (2015). 21G. Tanaka, T. Yamane, J. B. Héroux, R. Nakane, N. Kanazawa, S. Takeda,\nH. Numata, D. Nakano, and A. Hirose, “Recent advances in physical reservoir\ncomputing: A review,” Neural Netw. 115, 100 (2019). 22M. Lukoševicˇius and H. Jaeger, “Reservoir computing approaches to recurrent\nneural network training,” Comput. Sci. Rev. 3, 127–149 (2009). 23L. A. Thiede and U. Parlitz, “Gradient based hyperparameter optimization in\necho state networks,” Neural Netw. 115, 23–29 (2019). 24J. Yperman and T. Becker, “Bayesian optimization of hyper-parameters in\nreservoir computing,” arXiv:1611.05193 (2016).\n\n25S. Krishnagopal, M. Girvan, E. Ott, and B. R. Hunt, “Separation of chaotic\nsignals by reservoir computing,” Chaos 30, 023123 (2020). 26D. M. Wolpert and M. Kawato, “Multiple paired forward and inverse models\nfor motor control,” Neural Netw. 11, 1317–1329 (1998). 27J. Tani, M. Ito, and Y. Sugita, “Self-organization of distributedly represented\nmultiple behavior schemata in a mirror system: Reviews of robot experiments\nusing RNNPB,” Neural Netw. 17, 1273–1289 (2004). 28M. I. Rabinovich, P. Varona, A. I. Selverston, and H. D. Abarbanel, “Dynamical\nprinciples in neuroscience,” Rev. Mod. Phys. 78, 1213 (2006). 29G. J. Mpitsos and C. S. Cohan, “Convergence in a distributed nervous system:\nParallel processing and self-organization,” J. Neurobiol. 17, 517–545 (1986). 30I. R. Popescu and W. N. Frost, “Highly dissimilar behaviors mediated by a mul-\ntifunctional network in the marine mollusk Tritonia diomedea,” J. Neurosci. 22,\n1985–1993 (2002). 31H. Lu, “Chaotic attractors in delayed neural networks,” Phys. Lett. A 298,\n109–116 (2002). 32B. Bao, H. Qian, J. Wang, Q. Xu, M. Chen, H. Wu, and Y. Yu, “Numerical anal-\nyses and experimental validations of coexisting multiple attractors in Hopfield\nneural network,” Nonlinear Dyn. 90, 2359–2369 (2017). 33Z.-H. Guan, Q. Lai, M. Chi, X.-M. Cheng, and F. Liu, “Analysis of a new three-\ndimensional system with multiple chaotic attractors,” Nonlinear Dyn. 75, 331–343\n(2014). 34H. Richter, “Controlling chaotic systems with multiple strange attractors,” Phys.\nLett. A 300, 182–188 (2002). 35Y. A. Kuznetsov, Elements of Applied Bifurcation Theory (Springer Science &\nBusiness Media, 2013), Vol. 112. 36J. Guckenheimer and P. Holmes, Nonlinear Oscillations, Dynamical Systems,\nand Bifurcations of Vector Fields (Springer Science & Business Media, 2013),\nVol. 42. 37E. J. Doedel, T. F. Fairgrieve, B. Sandstede, A. R. Champneys, Y. A. Kuznetsov,\nand X. Wang, “Auto-07p: Continuation and bifurcation software for ordinary\ndifferential equations,” Technical Report, 2007. 38P. Tass, M. Rosenblum, J. Weule, J. Kurths, A. Pikovsky, J. Volkmann, A. Schnit-\nzler, and H.-J. Freund, “Detection of n: M phase locking from noisy data:\nApplication to magnetoencephalography,” Phys. Rev. Lett. 81, 3291 (1998). 39M. Rosenblum and A. Pikovsky, “Delayed feedback control of collective syn-\nchrony: An approach to suppression of pathological brain rhythms,” Phys. Rev. E\n70, 041904 (2004). 40P. A. Tass, C. Hauptmann, and O. V. Popovych, “Development of therapeu-\ntic brain stimulation techniques with methods from nonlinear dynamics and\nstatistical physics,” Int. J. Bifurcat. Chaos 16, 1889–1911 (2006). 41R. M. Harris-Warrick and E. Marder, “Modulation of neural networks for\nbehavior,” Annu. Rev. Neurosci. 14, 39–57 (1991).\n\nChaos 31, 013125 (2021); doi: 10.1063/5.0019974 © Author(s) 2021\n\n31, 013125-14\n\n"}