arXiv:1805.03362v3 [nlin.CD] 18 Jun 2018

Attractor Reconstruction by Machine Learning
Zhixin Lu,1, a) Brian R. Hunt,2 and Edward Ott3 1)Department of Bioengineering, University of Pennsylvania, Philadelphia, PA, 19104 2)Institute for Physical Science and Technology, University of Maryland, College Park, MD 20742 3)Institute for Research in Electronics and Applied Physics, University of Maryland, College Park, MD 20742
A machine-learning approach called “reservoir computing” has been used successfully for short-term prediction and attractor reconstruction of chaotic dynamical systems from time series data. We present a theoretical framework that describes conditions under which reservoir computing can create an empirical model capable of skillful short-term forecasts and accurate long-term ergodic behavior. We illustrate this theory through numerical experiments. We also argue that the theory applies to certain other machine learning methods for time series prediction.
A long-standing problem is prediction and analysis of data generated by a chaotic dynamical system whose equations of motion are unknown. Techniques based on delay-coordinate embedding have been successful for suﬃciently lowdimensional systems. Recently, machine-learning approaches such as reservoir computing have shown promise in treating a larger class of systems. We develop a theory of how prediction with reservoir computing or related machine-learning methods can “learn” a chaotic system well enough to reconstruct the long-term dynamics of its attractor.
I. INTRODUCTION
Reservoir computing1–4 is a machine-learning approach that has demonstrated success at a variety of tasks, including time series prediction5–8 and inferring unmeasured variables of a dynamical system from measured variables9,10. In this approach, a “reservoir” is a high-dimensional, non-autonomous (driven) dynamical system, chosen independently of the task. A particular task provides an input time series, and the reservoir state as a function of time is regarded as a “raw” output time series, which is post-processed to ﬁt the task. The post-processing function is determined, typically by linear regression, from a limited-time “training” data set consisting of the desired output time series for a given input time series.
Reservoir computing can be performed entirely in software, typically with an artiﬁcial neural network model, or with a physical reservoir; examples of the latter include a bucket of water11, an electronic circuit with a time delay12, a ﬁeld-programmable gate array (FPGA)13, an optical network of semiconductor lasers14, and an optic-electronic phasedelay system15. Other machine-learning techniques, including deep learning16,17, attempt to optimize internal system parameters to ﬁt the training data; doing so requires a mathematical model of the machine-learning system. By contrast, reservoir computing does not require a model for the reservoir, nor the ability to alter the reservoir dynamics, because it seeks only to optimize the parameters of the post-processing function. The ability to use a physical reservoir as a “black box” allows for various potential advantages over other machine-learning techniques, including greatly enhanced speed.
In this article, we consider the task of predicting future measurements from a deterministic dynamical system, whose equations of motion are unknown, from limited time series data. We describe a general framework that includes the reservoir computing prediction
a)Electronic mail: zhixinlu@seas.upenn.edu.

2

method proposed by Jaeger and Haas5. With appropriate modiﬁcations, the same frame-
work applies to other machine-learning methods for time series prediction (including an LSTM approach18), as we discuss further in Sec. IV. We assume the vector u(t) of mea-
surements to be a function h of the ﬁnite-dimensional system state s(t),

u(t) = h(s(t)).

(1)

For simplicity, we assume that there is no measurement noise, though our discussion below could be modiﬁed for the case that Eq. (1) is an approximation. We do not assume that h is invertible, nor that h or s is known in practice. Training data consists of a ﬁnite time series {u(t)} of measurements. We predict future values of u(t) by a sequence of three steps, which we call listening, training, and predicting.
Listening consists of using the training time series as input to the reservoir, which we model as a discrete time deterministic process:

r(t + τ ) = f [r(t), u(t)].

(2)

Here r(t) is the reservoir state, τ is a time increment, and we assume f to be a diﬀerentiable function. We emphasize that in practice, a formula for f need not be known; only its outputs are used for training and prediction. For convenience, we assume that the full reservoir state r(t) can be measured or computed, though our arguments can be modiﬁed easily for the more general case that the reservoir output is a function of its internal state. We call Eq. (2) the “listening reservoir”.
Training consists of determining a post-processing function ψˆ that, when applied to the reservoir output r(t+τ ), estimates the next input u(t+τ ). (We view ψˆ as an approximation to an “ideal” post-processing function ψ, to be introduced in Sec. II B.) Thus, the goal of training is to ﬁnd ψˆ such that ψˆ (r(t + τ )) ≈ u(t + τ ), or equivalently,

ψˆ (r(t)) ≈ u(t),

(3)

for t large enough that the listening reservoir (2) has evolved beyond transient dynamics. We compute ψˆ by a ﬁtting procedure, such as linear regression, on the training time series {u(t)} and the corresponding time series {r(t)} determined from the listening reservoir (2).
Predicting then proceeds by modifying the reservoir to run autonomously with a feedback loop, replacing its input [u(t) in Eq. (2)] with its post-processed output from the previous time increment:

ˆr(t + τ ) = f [ˆr(t), ψˆ (ˆr(t))].

(4)

We call Eq. (4) the “predicting reservoir”. When initialized (from the listening reservoir state) with ˆr(t0) = r(t0), iterating the predicting reservoir yields a time series {ψˆ (ˆr(t0 + τ )), ψˆ (ˆr(t0 + 2τ )), . . .} of predictions for future measurements {u(t0 + τ ), u(t0 + 2τ ), . . .}. (Our notation reﬂects the fact that for t > t0, the predicting reservoir state ˆr(t) estimates the state r(t) that would result from evolving the listening reservoir (2) with the future
measurements.)
The reservoir prediction method we have described has been shown to produce successful short-term forecasts for a variety of dynamical systems5,7,8. If the system has a chaotic attractor, then, as for any imperfect model, the prediction error ψˆ (ˆr(t)) − u(t) cannot remain small for t t0. However, in some cases, the long-term time series {ψˆ (ˆr(t))} continues to behave like the measurements from a typical trajectory on the attractor, and
in this sense the predicting reservoir (4) approximately reproduces the ergodic properties of the dynamical system that generated the measurements8. We refer to this ability, often
called attractor reconstruction, as replication of the “climate”.
In this article, we develop and illustrate a theory of how reservoir prediction is able to
“learn” the dynamics of a system well enough to produce both accurate short-term forecasts
and accurate long-term climate. We make use of the notion of generalized synchronization19–22, which in our context means that the reservoir state r(t) becomes asymptotically

3

a continuous function φ of s(t), in the limit that the listening reservoir (2) is run inﬁnitely long. In Sec. II, we argue that the following four conditions are suﬃcient for both short-term prediction and attractor/climate replication.
1. The listening reservoir (2) achieves generalized synchronization with the process {s(t)}, so that r(t) ≈ φ(s(t)) for a continuous function φ, within the time interval covered by the training time series.
2. The synchronization function φ is one-to-one, or at least carries enough information about its input to recover u(t) = h(s(t)) from φ(s(t)).
3. Training is successful in ﬁnding a function ψˆ such that Eq. (3) holds, or equivalently in view of generalized synchronization, that ψˆ (φ(s(t))) ≈ h(s(t)).
4. The attractor approached by the listening reservoir is also stable for the predicting reservoir (4).
Conditions 1–3 enable short-term prediction. Condition 4 ensures that the climate established by generalized synchronization of the listening reservoir is preserved when its input is replaced by a feedback term to form the predicting reservoir. One of the main points of Sec. II is to precisely formulate the stability condition described in Condition 4.
We remark that generalized synchronization of the listening reservoir6,10 is related to the “echo state property”1,23, which states that an inﬁnite history of inputs {u(t − τ ), u(t − 2τ ), . . .} uniquely determines r(t), subject to the condition that the trajectory {r(t)} is bounded. Indeed, if {s(t)} is a trajectory of an invertible dynamical system, then the past inputs are functions of s(t), so the echo state property implies that if the listening reservoir (2) has run for an inﬁnite period of time in a bounded domain, then r(t) is a function of s(t) [though it does not imply that this function is continuous]. We believe that for the reservoir prediction method we described, it is desirable (though not strictly necessary) to have the echo state property and generalized synchronization. In Sec. II A, we show why both properties hold if the listening reservoir is uniformly contracting as a function of r, and that we can quantify the amount of transient time it takes for the reservoir to achieve the approximation r(t) ≈ φ(s(t)) of Condition 1.
Conditions 2 and 3 are signiﬁcantly more diﬃcult to ensure a priori. In Sec. II B, we argue why it is plausible that these conditions can be achieved. In Secs. II C and II D, we describe the consequences of Conditions 1-3 for short-term prediction, and formulate more precisely the stability criterion of Condition 4 that determines whether the correct attractor and climate are approximately reproduced by the long-term dynamics of the predicting reservoir (4). In Sec. II E, we describe how a model for the reservoir dynamics can be used to compute Lyapunov exponents that reﬂect climate stability.
In Sec. III, we give examples of short-term state and long-term climate predictions using the Lorenz equations as our input system. In addition to a case where the climate is approximated well, we show a case where the predicted climate is inaccurate, though the short-term forecast is still reasonably accurate. We compute the Lyapunov exponents of the predicting reservoir (4), and show that the transition from accurate climate to inaccurate climate corresponds to a Lyapunov exponent crossing zero. When this Lyapunov exponent is positive but close to zero, the reservoir prediction remains close to the correct climate for a transient period, and we relate the average duration of this transient to the value of the Lyapunov exponent.

II. THEORY

We consider the application of the reservoir prediction method described in the introduction to a time series {u(t)} that is a function h of a trajectory {s(t)} of the dynamical system

s(t + τ ) = g(s(t)),

(5)

4

where g is diﬀerentiable and invertible, and we assume that s(t) evolves on a bounded attractor A. In preparation for training and prior to prediction, the reservoir state r(t) evolves according to the listening reservoir (2). The system described by Eqs. (5) and (2), coupled by Eq. (1), is often called a drive-response, skew-product, or one-way coupled system. The coupled system dynamics are illustrated by Fig. 1. We next consider the evolution of the coupled system as t → ∞.

... g s(t) g s(t + ⌧) g ...

h

h

...

u(t) u(t + ⌧ )

...

... f r(t) f r(t + ⌧) f ...

FIG. 1. Drive-response system dynamics, with the drive state s(t) coupled to the listening reservoir state r(t) through the measurement vector u(t).

A. Listening and Generalized Synchronization
The goal of training can be regarded as ﬁnding a post-processing function ψˆ such that ψˆ (r(t)) is in approximate identical synchronization20 with u(t) = h(s(t)), when r(t) is evolved with the listening reservoir (2). The desired relationship u(t) ≈ ψˆ (r(t)) can also be thought of as approximate generalized synchronization between u(t) [or the underlying state s(t)] and r(t). The existence of such a relationship would be implied by stochastic synchronization19, which in our context means a one-to-one correspondence between r(t) and s(t) in the limit t → ∞. However, in drive-response systems, the deﬁnition of generalized synchronization21,22 requires only that the response state be asymptotically a function of the drive state: in our case, that there is a continuous function φ such that r(t)−φ(s(t)) → 0 as t → ∞. The existence of such a φ is typically easier to establish than its invertibility. Next, we describe conditions on the reservoir system f that guarantee generalized synchronization.
Though weaker conditions are possible, we assume uniform contraction for f , as is often the case in practice. By uniform contraction, we mean that there is some ρ < 1 such that for all r1, r2, and u we have that |f (r1, u) − f (r2, u)| < ρ|r1 − r2|. It then follows that two trajectories {r1(t), u(t)} and {r2(t), u(t)} of (2) with the same input time series approach each other exponentially: |r1(t) − r2(t)| ≤ |r1(0) − r2(0)|ρt/τ . Thus, for a given input time series {u(t)}, the reservoir state r(t) is asymptotically independent of its initial state; this is essentially what Jaeger1 called the “echo state property”. Furthermore, because g is invertible and A is bounded, and due to results of Hirsch, Pugh, and Shub24,25 (a direct proof is given by Stark26), uniform contraction implies generalized synchronization, as deﬁned above. (In general, the synchronization function φ cannot be determined analytically from f , g, and h.) A weaker form of generalized synchronization can also be guaranteed26 from the non-uniform contraction implied by negative conditional Lyapunov exponents.
We remark that if the listening reservoir (2) is uniformly contracting, then r(t) − φ(s(t)) converges to zero exponentially. If the designer of the reservoir can guarantee a speciﬁc contraction rate ρ, this determines the convergence rate, so that the amount of transient time needed to make the approximation r(t) ≈ φ(s(t)) accurate can be known in practice.
Generalized synchronization implies that the set of (s, r) such that s is on its attractor A and r = φ(s) is an attractor for the drive-response system given by Eqs. (5), (1), and (2). Below we will use the fact that this set is invariant: r(t) = φ(s(t)) implies r(t + τ ) = φ(s(t + τ )).

5

B. Training
Recall that training seeks a function ψˆ that predicts the current measurement vector u(t) from the current listening reservoir state r(t) [which is computed from past measurements], and that when generalized synchronization is achieved, accuracy of this prediction is equivalent to ψˆ (φ(s(t))) ≈ h(s(t)). For the rest of Section II, we assume that there is a function ψ deﬁned on φ(A) such that ψ(φ(s)) = h(s) for all s in A. This assumption means that in the asymptotic limit of generalized synchronization, the listening reservoir state r(t) = φ(s(t)) uniquely determines u(t) = h(s(t)). The goal of training can then be described as ﬁnding a function ψˆ deﬁned on the state space of the reservoir that approximates ψ on the set φ(A). We summarize our notation in Table I.

Dynamical System to be Predicted

s(t)

System state

g : s(t) → s(t + τ )

System evolution

A

Attractor for s(t)

Measurements

u(t)

Measurement vector

h : s(t) → u(t)

Measurement function

Reservoir

r(t)

Listening reservoir state

f : [r(t), u(t)] → r(t + τ ) Listening reservoir evolution

ˆr(t) uˆ(t) = ψˆ (ˆr(t))

Predicting reservoir state Predicted measurements

f : [ˆr(t), uˆ(t)] → ˆr(t + τ ) Predicting reservoir evolution

Generalized Synchronization

φ : s → r for s in A

Synchronization function

ψ : r → u for r in φ(A) Ideal post-processing function

ψˆ : ˆr(t) → uˆ(t)

Actual post-processing function

TABLE I. Summary of Notation

Though the existence of ψ is not strictly necessary for the reservoir to make useful predictions, if no such ψ exists, then it seems unlikely that training can successfully achieve the desired approximation ψ(φ(s(t))) ≈ h(s(t)), and thus unlikely that u(t) can be approximated as a function of the reservoir state during either listening or predicting. The existence of ψ is guaranteed if φ is one-to-one on A; then ψ = h ◦ φ−1. Furthermore, if h is one-to-one on A (in other words, the measurements at a given time determine the system state), then φ must be one-to-one on A in order for ψ to exist. Thus, we propose that a goal of reservoir design should be to yield a one-to-one synchronization function φ for a variety of input systems. In practice, having a suﬃciently high-dimensional reservoir may suﬃce; embedding results27,28 imply that if the dimension of the reservoir state r is more than twice the dimension of A, functions from A to the reservoir state space are typically one-to-one. We note that in practice, the dimension of r must be much larger than twice the dimension of A in order to provide a suitable basis for approximating ψ, in the sense described below.
Careful consideration of conditions under which training is successful in determining an accurate approximation ψˆ to ψ is beyond the scope of our theory. However, we argue that success is plausible if the training time series is suﬃciently long that the trajectory {s(t)} well samples its attractor A, if the dimension of the reservoir state r(t) is suﬃciently high, and if the dynamics of the coordinates of r(t) are suﬃciently heterogeneous. If, for example, training uses linear regression of {u(t)} = {h(s(t))} versus {r(t)}, then since r(t) ≈ φ(s(t)), the coordinates of the vector-valued function φ(s) can be thought of “basis functions”6;

6
training seeks a linear combination ψˆ of these basis functions that approximates h(s) on A. A suitable basis for training (using a linear or nonlinear combination) is plausible if the listening reservoir yields a suﬃciently large variety of responses to its input.

C. Prediction and Attractor Reconstruction

After training determines the post-processing function ψˆ , prediction proceeds by initial-
izing ˆr(t0) = r(t0) and evolving ˆr(t) for t ≥ t0 according to the predicting reservoir (4). The reservoir state r(t0) is determined by evolving the listening reservoir (2) for an interval of time preceding t0; this could be the time interval used for training, or it could be a later time interval that uses inputs {u(t)} measured after training (we call this feature “training reusability”29). We assume that the listening time preceding t0 is suﬃciently long to achieve generalized synchronization, so that ˆr(t0) = r(t0) ≈ φ(s(t0)) is near φ(A). For t ≥ t0, the predicted value of u(t) is

uˆ(t) = ψˆ (ˆr(t)).

(6)

Figure 2 depicts the dynamics of the predicting reservoir (4).

... ...

f

uˆ(t)
ˆ
<latexit sha1_base64="2us6ZcueJpt8J1LHVBZHf/82D+FhcM6wQMq6yDtKlcQ5gsBzeCPwY0=">AAAB/XicbVCB9PTSs8MwGHPEx3S9/Ok+re5/CqynAhmSGJCDwgqEJTK6YPq1ZoQtG62GCXhjbxGuVYiNK10gULhSNVNj0uyO00sVThU0qnSsChq2MkMKvqPrhEBqv7HAhwQA8GeKoF+5h2zBcj/4g1F/wC2dgN7tQBcNpxL+lE0P9N3737y/eccjLUCs16NUGdlpXwavcqL72St0hvcLWKl65VZlb6Wv0bVGlx7ufb2eN/zYaut3ntd32K7s5kTkIoJSC5YJeeFCkIz7IAVVoagUMY0ZFQdTzTT1SPnNnSVCRuSVHBACeUchtIo8P2hwdfe1G3347z6XlsUiLFBRGX38eVpgR9SPE8iZS9owyS2JlGMsMDdZJSGz6zt7jw7Bflgj/npX3pAD8wFUiI91QigcM3ZmepfYKujOresX2XBVWqnz6goRwoBk5d4Rkn7pJNZoX4a+4mahj9xA/0XCgz0Zq3n9f6sYTUzK8ySmOAEpsNIO1F5agqhWp3ddqSu7ke+doYgaRk1YJ4THzVMei8wT7NGEeUKkpyAHju30U6Z6eh0ADsedUUo+IfSkrkI/xh/iGdHG8CMVQEoUESYLyNFENRIpdNr1ONF8Zb/ObY+5QVoEUcU5UM4xMl5gkPg13LVxOXziXPi9H5+35U7xUHzFH3V78OEROJepVpKpsgjn0HokS4jfjiSjCEeEot6YANKEFTjFKKgik+WcLgOQBTICQQhzLWaRrEJZCY3IEmMJSNYoW10VaTKA5ns1S+3SN8kvEzvxePsvdrlHlpzZWdqRaqMmKj5CisiwDQIQdkwcDgKGdNQwhC3lNxowwDAm0r0gwBQtUSCBOBTzzDCIMw7RxNY4DA9aa/TW9gW/qV9sTvUVdnLv1km9xEnFHa/77qAzeBv/87BAq+qv2gYBXhQ4=y=Z<w/wl=a=t<e/xliat>exit>
ˆr(t)
<latexit sha1_base64="NLRavC7UCxZDjp8A9ibptnwum6M=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSLUS0lEUG9FLx4rGFtoQtlsN+3SzSbsvgghxL/ixYOKV/+IN/+NmzYHbR1YGGbe481OkHCmwLa/jZXVtfWNzdpWfXtnd2/fPDh8UHEqCXVJzGPZD7CinAnqAgNO+4mkOAo47QXTm9LvPVKpWCzuIUuoH+GxYCEjGLQ0NBveBEPuRRgmQZjLomjB6dBs2m17BmuZOBVpogrdofnljWKSRlQA4VipgWMn4OdYAiOcFnUvVTTBZIrHdKCpwBFVfj4LX1gnWhlZYSz1E2DN1N8bOY6UyqJAT5Yp1aJXiv95gxTCSz9nIkmBCjI/FKbcgtgqm7BGTFICPNMEE8l0VotMsMQEdF91XYKz+OVl4p61r9r23Xmzc121UUNH6Bi1kIMuUAfdoi5yEUEZekav6M14Ml6Md+NjPrpiVDsN9AfG5w9i2ZTK</latexit>

f

uˆ(t + ⌧ )

ˆ
<latexit sha1_base64="2us6ZcueJpt8J1LHVBZHf/82D+FhcM6wQMq6yDtKlcQ5gsBzeCPwY0=">AAAB/XicbVCB9PTSs8MwGHPEx3S9/Ok+re5/CqynAhmSGJCDwgqEJTK6YPq1ZoQtG62GCXhjbxGuVYiNK10gULhSNVNj0uyO00sVThU0qnSsChq2MkMKvqPrhEBqv7HAhwQA8GeKoF+5h2zBcj/4g1F/wC2dgN7tQBcNpxL+lE0P9N3737y/eccjLUCs16NUGdlpXwavcqL72St0hvcLWKl65VZlb6Wv0bVGlx7ufb2eN/zYaut3ntd32K7s5kTkIoJSC5YJeeFCkIz7IAVVoagUMY0ZFQdTzTT1SPnNnSVCRuSVHBACeUchtIo8P2hwdfe1G3347z6XlsUiLFBRGX38eVpgR9SPE8iZS9owyS2JlGMsMDdZJSGz6zt7jw7Bflgj/npX3pAD8wFUiI91QigcM3ZmepfYKujOresX2XBVWqnz6goRwoBk5d4Rkn7pJNZoX4a+4mahj9xA/0XCgz0Zq3n9f6sYTUzK8ySmOAEpsNIO1F5agqhWp3ddqSu7ke+doYgaRk1YJ4THzVMei8wT7NGEeUKkpyAHju30U6Z6eh0ADsedUUo+IfSkrkI/xh/iGdHG8CMVQEoUESYLyNFENRIpdNr1ONF8Zb/ObY+5QVoEUcU5UM4xMl5gkPg13LVxOXziXPi9H5+35U7xUHzFH3V78OEROJepVpKpsgjn0HokS4jfjiSjCEeEot6YANKEFTjFKKgik+WcLgOQBTICQQhzLWaRrEJZCY3IEmMJSNYoW10VaTKA5ns1S+3SN8kvEzvxePsvdrlHlpzZWdqRaqMmKj5CisiwDQIQdkwcDgKGdNQwhC3lNxowwDAm0r0gwBQtUSCBOBTzzDCIMw7RxNY4DA9aa/TW9gW/qV9sTvUVdnLv1km9xEnFHa/77qAzeBv/87BAq+qv2gYBXhQ4=y=Z<w/wl=a=t<e/xliat>exit>
ˆr(t + ⌧ )
<latexit sha1_base64="G0mrjRsBEwTLSzDgz+U18zjta0Q=">AAACAHicbVBNS8NAEN3Ur1q/ol4EL4tFqAglEUG9Fb14rGBsoQlls920Szcf7E6EEuLFv+LFg4pXf4Y3/42bNgetPhh4vDfDzDw/EVyBZX0ZlYXFpeWV6mptbX1jc8vc3rlTcSopc2gsYtn1iWKCR8wBDoJ1E8lI6AvW8cdXhd+5Z1LxOLqFScK8kAwjHnBKQEt9c88dEcjckMDIDzKZ5w04doGkR32zbjWtKfBfYpekjkq0++anO4hpGrIIqCBK9WwrAS8jEjgVLK+5qWIJoWMyZD1NIxIy5WXTD3J8qJUBDmKpKwI8VX9OZCRUahL6urM4Vc17hfif10shOPcyHiUpsIjOFgWpwBDjIg484JJREBNNCJVc34rpiEhCQYdW0yHY8y//Jc5J86Jp3ZzWW5dlGlW0jw5QA9noDLXQNWojB1H0gJ7QC3o1Ho1n4814n7VWjHJmF/2C8fENBa2WzQ==</latexit>

f

... ...

FIG. 2. Predicting reservoir dynamics, with the listening reservoir input u(t) replaced by the estimate uˆ(t) determined from the predicting reservoir state ˆr(t).

Consider now the idealized scenario that our approximations are instead exact relations ψˆ = ψ on φ(A), and ˆr(t0) = r(t0) = φ(s(t0)). Suppose hypothetically that the measurements {u(t)} for t ≥ t0 (these are the values we want to predict in practice) are available, so that we can evolve both the listening reservoir (2) depicted in Fig. 1, and the predict-
ing reservoir (4) depicted in Fig. 2, and compare their outputs. Then we claim that the
two reservoirs agree exactly: ˆr(t) = r(t) and uˆ(t) = u(t) for all t ≥ t0. First notice that uˆ(t0) = ψˆ (ˆr(t0)) = ψ(φ(s(t0)) = h(s(t0)) = u(t0). Then ˆr(t0 + τ ) = f [ˆr(t0), uˆ(t0)] = f [r(t0), u(t0)] = r(t0 + τ ), and r(t0 + τ ) = φ(s(t0 + τ )) due to generalized synchronization. Similarly, uˆ(t0 + τ ) then equals u(t0 + τ ), so ˆr(t0 + 2τ ) = r(t0 + 2τ ) = φ(s(t0 + 2τ )), etc. This agreement between the trajectories also shows that φ(A) is an invariant set for the
idealized predicting reservoir

r(t + τ ) = f [r(t), ψ(r(t))],

(7)

and that its dynamics, observed through ψ, are equivalent to the dynamics of A observed through h.
Thus, if the time series {u(t)} of measurements has enough information to reconstruct the attractor A, then we can regard φ(A) and the idealized predicting reservoir (7) as an exact reconstruction of A and its dynamics. When the approximation ψˆ ≈ ψ is not exact on φ(A), the actual predicting reservoir (4) is still initialized near φ(A), but φ(A) is only approximately invariant. The better the approximation, the more accurate the predictions uˆ(t) ≈ u(t) will be, at least in the short term. However, if the system (5) that generates the measurements {u(t)} is chaotic, the prediction error uˆ(t) − u(t) will typically grow exponentially as t increases.
Nonetheless, it remains possible that uˆ(t) will maintain a climate similar to u(t) in the long term. This will happen if (and practically speaking, only if) the predicting reservoir trajectory {ˆr(t)} remains close to φ(A) for all time, and its attractor has a similar climate to that of the idealized predicting reservoir on φ(A). In this sense, climate replication

7
(attractor reconstruction) relies on both state-space stability and structural stability of the predicting reservoir near the idealized reconstructed attractor φ(A).
Structural stability is diﬃcult to ensure rigorously, but in practice small perturbations of the dynamics near an attractor tend to yield small perturbations to the climate. Thus, we argue that climate replication is likely if φ(A), which according to our assumptions is invariant for the idealized predicting reservoir, is also attracting, in the sense described below.
D. Stability and Lyapunov Exponents
Recall that generalized synchronization implies that the set φ(A) is attracting for the listening reservoir (2), when driven by u(t) = h(s(t)) where s(t) evolves on A. Whether φ(A) is attracting for the predicting reservoir is complicated by the fact that it is invariant only in the idealized case ψˆ = ψ, and that ψ is deﬁned only on φ(A), so that the idealized predicting reservoir (7) is also deﬁned only on φ(A). For its stability to be well-deﬁned, the domain of ψ must be extended to a neighborhood of φ(A), and whether φ(A) is attracting depends on how the extension is chosen.
Thus, the suitability of the empirically determined function ψˆ for climate prediction depends not only on how well it approximates ψ on φ(A), but also on how it behaves near φ(A). For a particular ψˆ , we consider hypothetically a particular extension of ψ such that ψˆ ≈ ψ near φ(A). This extension gives the idealized predicting reservoir a full set of Lyapunov exponents on φ(A), some of which correspond to inﬁnitesimal perturbations tangent to φ(A) and some of which correspond to inﬁnitesimal perturbations transverse to φ(A). Then φ(A) is attracting if the transverse Lyapunov exponents are all negative, and is unstable if there is a positive transverse Lyapunov exponent.
If the generalized synchronization function φ is one-to-one and diﬀerentiable, then the tangential Lyapunov exponents of the system (5) on A are reproduced as the tangential Lyapunov exponents of the idealized predicting reservoir on φ(A). Generalized synchronization does not always yield a diﬀerentiable φ26,30, but even when diﬀerentiability cannot be guaranteed, it is possible in practice to reproduce much of the Lyapunov spectrum of A, including negative Lyapunov exponents in some cases, with a predicting reservoir8.
We remark that unlike the conditional Lyapunov exponents for a drive-response system (such as the listening reservoir), which correspond to perturbations of the response system state, for the predicting reservoir it is not clear in advance which perturbations correspond to transverse Lyapunov exponents. However, in a numerical experiment where the equations for the driving system (5) and the reservoir are known, the existence or absence of a positive transverse Lyapunov exponent can be inferred by computing all of the positive Lyapunov exponents of the predicting reservoir and eliminating those that are Lyapunov exponents of A.
E. Computation of Lyapunov Exponents
We now describe how to estimate the Lyapunov exponents of the idealized predicting reservoir (7) on φ(A), for a particular extension of ψ to a neighborhood of φ(A), from its empirical approximation ψˆ . To do so, we assume that we have a formula for f , so that we can compute its Jacobian matrix. (We emphasize that we estimate the Lyapunov exponents in order to corroborate the theory we have presented; their computation, and a formula for f , are not needed for the reservoir prediction method we have described.) If climate replication is successful, we can simply generate a long trajectory of the predicting reservoir (4), and use it to compute the Lyapunov exponents of the trajectory8. However, this trajectory cannot be expected to remain close to φ(A) if the set is unstable. Nonetheless, if we have a suﬃciently long time time series {u(t)} of measurements, we can estimate the Lyapunov exponents of φ(A), whether or not it is stable, as follows.

8

First, we use the time series {u(t)} to generate a trajectory {r(t)} of the listening reservoir (2); as we have argued, r(t) will approach φ(A) under the conditions for generalized synchronization. Then along this trajectory, which is an approximate trajectory for the predicting reservoir, we compute Lyapunov exponents using the Jacobian matrix of the predicting reservoir (4).

III. NUMERICAL EXPERIMENTS

In this section, we give examples of short-term state and long-term climate predictions for the Lorenz system31, with standard parameter values that yield chaotic trajectories:

dx/dt = 10(y − x),

dy/dt = x(28 − z) − y,

(8)

dz/dt = xy − 8z/3.

We consider the case where the measurement function h is the identity, so that u(t) = s(t) = [x(t), y(t), z(t)]T . For the reservoir, we use an artiﬁcial neural network similar to the one used by Jaeger and Haas5; our listening reservoir [a continuous-time version of Eq. (2)]
evolves according to

d dt

r(t)

=

γ[−r(t)

+

tanh(Mr(t)

+

σWinu(t))],

(9)

where r is an N -dimensional vector, γ is a scalar, M is an adjacency matrix representing internal network connections. The matrix σWin consists of “input weights”; in our numerical results, we will ﬁx Win and vary the scalar input strength σ. The vector function tanh is computed by applying the scalar hyperbolic tangent to each coordinate of its input vector. We compute trajectories of both the Lorenz and reservoir systems using the 4th order Runge-Kutta method with time step τ = 0.001. We will show cases where climate replication (attractor reconstruction) succeeds and where it fails, and compare the results with Lyapunov exponents we compute for the predicting reservoir.

Win
<latexit sha1_base64="oKaFNYvHzwQCK8stuEtrqNshuNc=">AAACBHicbVBNS8NAFNzUr1q/oh71ECyCp5KIoN6KXjxWMLbQhLDZbtqlu0nYfRFLyMWLf8WLBxWv/ghv/hs3bRFtHVgYZt5j30yYcqbAtr+MysLi0vJKdbW2tr6xuWVu79yqJJOEuiThieyEWFHOYuoCA047qaRYhJy2w+Fl6bfvqFQsiW9glFJf4H7MIkYwaCkw9z3F+gJ7AsMgjPJ2EeQe0HvIWVwUgVm3G/YY1g9xZkkdTdEKzE+vl5BM0BgIx0p1HTsFP8cSGOG0qHmZoikmQ9ynXU1jLKjy83GKwjrUSs+KEqlfDNZY/b2RY6HUSIR6srxWzXql+J/XzSA683WgNAMak8lHUcYtSKyyEqvHJCXAR5pgIpm+1SIDLDEBXVxNlzAXeZ64x43zhnN9Um9eTNuooj10gI6Qg05RE12hFnIRQQ/oCb2gV+PReDbejPfJaMWY7uyiPzA+vgF5qZlp</latexit>

Woutq <latexit sha1_base64="7OH6mI+fHpPJQzzteEl8CVf/V3M=">AAACCXicdVDLSsNAFJ3UV62vqEs3o0VwVVIR1F3RjcsKxhaaECbTSTt08nDmRiwhazf+ihsXKm79A3f+jZM+oL4OXDiccy/33uMngiuwrE+jNDe/sLhUXq6srK6tb5ibW9cqTiVlNo1FLNs+UUzwiNnAQbB2IhkJfcFa/uC88Fu3TCoeR1cwTJgbkl7EA04JaMkzd52QQN8PslbuZQ6wO8jiFPJ8Kt/knlmt16wRsPWLTK0qmqDpmR9ON6ZpyCKggijVqVsJuBmRwKlgecVJFUsIHZAe62gakZApNxu9kuN9rXRxEEtdEeCROjuRkVCpYejrzuJC9dMrxL+8TgrBiZvxKEmBRXS8KEgFhhgXueAul4yCGGpCqOT6Vkz7RBIKOr3KbAj/E/uwdlqrXx5VG2eTNMpoB+2hA1RHx6iBLlAT2Yiie/SIntGL8WA8Ga/G27i1ZExmttE3GO9f7rWb5g==</latexit>

u(t) <latexit sha1_base64="LA9BARvHiKn3oIx0/iAgxiS/KAM=">AAAB83icbVDLSsNAFL3xWeur6tLNYBHqpiQiqLuiG5cVjC20oUymk3bo5OHMTaGEfocbFypu/Rl3/o2TNgttPTBwOOde7pnjJ1JotO1va2V1bX1js7RV3t7Z3duvHBw+6jhVjLsslrFq+1RzKSLuokDJ24niNPQlb/mj29xvjbnSIo4ecJJwL6SDSASCUTSS1w0pDv0gS6c1POtVqnbdnoEsE6cgVSjQ7FW+uv2YpSGPkEmqdcexE/QyqlAwyaflbqp5QtmIDnjH0IiGXHvZLPSUnBqlT4JYmRchmam/NzIaaj0JfTOZh9SLXi7+53VSDK68TERJijxi80NBKgnGJG+A9IXiDOXEEMqUMFkJG1JFGZqeyqYEZ/HLy8Q9r1/XnfuLauOmaKMEx3ACNXDgEhpwB01wgcETPMMrvFlj68V6tz7moytWsXMEf2B9/gAJi5HQ</latexit>

r(t) 2 RN <latexit sha1_base64="KVZU0h/fd5YvLRoBNuJF2jBS1zI=">AAACBXicbVDLSsNAFL3xWesr6lIXwSLUTUlE0GXRjSupYh/QxDKZTtqhk0mYmQglZOPGX3HjQhG3/oM7/8ZJm4W2Hhg4c8693HuPHzMqlW1/GwuLS8srq6W18vrG5ta2ubPbklEiMGniiEWi4yNJGOWkqahipBMLgkKfkbY/usz99gMRkkb8To1j4oVowGlAMVJa6pkHbojU0A9SkVXVsUv59O+nt9n9dc+s2DV7AmueOAWpQIFGz/xy+xFOQsIVZkjKrmPHykuRUBQzkpXdRJIY4REakK6mHIVEeunkisw60krfCiKhH1fWRP3dkaJQynHo68p8Rznr5eJ/XjdRwbmXUh4ninA8HRQkzFKRlUdi9akgWLGxJggLqne18BAJhJUOrqxDcGZPnietk5pj15yb00r9ooijBPtwCFVw4AzqcAUNaAKGR3iGV3gznowX4934mJYuGEXPHvyB8fkDud6Ysw==</latexit>

FIG. 3. Listening reservoir based on an artiﬁcial neural network with N neurons. The input vector u(t) ∈ R3 is mapped to the reservoir state space RN by the input weight matrix σWin, and the resulting reservoir state is mapped to R3 by the post-processing function ψˆ = Woutq.

We consider post-processing functions of the form ψˆ (r) = Woutq(r), where q(r) is the 2N -dimensional vector consisting of the N coordinates of r followed by their squares, and
the “output weight” matrix Wout is determined by a linear regression procedure described below. The listening reservoir (9) and the post-processing function are illustrated as an
input-output system in Fig. 3. The goal of training is that the post-processed output
Woutq(r(t + τ )) based on input up to time t estimates the subsequent input u(t + τ ). Once Wout is determined, the external input can be replaced in a feedback loop by the postprocessed output to form the predicting reservoir, as depicted in Fig. 4. The predicting
reservoir evolves according to

d dt

ˆr(t)

=

γ[−ˆr(t)

+

tanh(Mˆr(t)

+

σWinWoutq(ˆr(t))],

(10)

and the predicted value of u(t) is uˆ(t) = ψˆ (ˆr(t)) = Woutq(ˆr(t)).

Win
<latexit sha1_base64="oKaFNYvHzwQCK8stuEtrqNshuNc=">AAACBHicbVBNS8NAFNzUr1q/oh71ECyCp5KIoN6KXjxWMLbQhLDZbtqlu0nYfRFLyMWLf8WLBxWv/ghv/hs3bRFtHVgYZt5j30yYcqbAtr+MysLi0vJKdbW2tr6xuWVu79yqJJOEuiThieyEWFHOYuoCA047qaRYhJy2w+Fl6bfvqFQsiW9glFJf4H7MIkYwaCkw9z3F+gJ7AsMgjPJ2EeQe0HvIWVwUgVm3G/YY1g9xZkkdTdEKzE+vl5BM0BgIx0p1HTsFP8cSGOG0qHmZoikmQ9ynXU1jLKjy83GKwjrUSs+KEqlfDNZY/b2RY6HUSIR6srxWzXql+J/XzSA683WgNAMak8lHUcYtSKyyEqvHJCXAR5pgIpm+1SIDLDEBXVxNlzAXeZ64x43zhnN9Um9eTNuooj10gI6Qg05RE12hFnIRQQ/oCb2gV+PReDbejPfJaMWY7uyiPzA+vgF5qZlp</latexit>

Woutq <latexit sha1_base64="7OH6mI+fHpPJQzzteEl8CVf/V3M=">AAACCXicdVDLSsNAFJ3UV62vqEs3o0VwVVIR1F3RjcsKxhaaECbTSTt08nDmRiwhazf+ihsXKm79A3f+jZM+oL4OXDiccy/33uMngiuwrE+jNDe/sLhUXq6srK6tb5ibW9cqTiVlNo1FLNs+UUzwiNnAQbB2IhkJfcFa/uC88Fu3TCoeR1cwTJgbkl7EA04JaMkzd52QQN8PslbuZQ6wO8jiFPJ8Kt/knlmt16wRsPWLTK0qmqDpmR9ON6ZpyCKggijVqVsJuBmRwKlgecVJFUsIHZAe62gakZApNxu9kuN9rXRxEEtdEeCROjuRkVCpYejrzuJC9dMrxL+8TgrBiZvxKEmBRXS8KEgFhhgXueAul4yCGGpCqOT6Vkz7RBIKOr3KbAj/E/uwdlqrXx5VG2eTNMpoB+2hA1RHx6iBLlAT2Yiie/SIntGL8WA8Ga/G27i1ZExmttE3GO9f7rWb5g==</latexit>

9

ˆr(t) 2 RN <latexit sha1_base64="B792MrZMKAZF/Q4Pz5eI9eBtlE0=">AAACC3icbVBNS8NAEN34WetX1KOX0CLUS0lE0GPRiyepYj+giWWz3bRLN5uwOxFKyN2Lf8WLB0W8+ge8+W/ctDlo64OBx3szzMzzY84U2Pa3sbS8srq2Xtoob25t7+yae/ttFSWS0BaJeCS7PlaUM0FbwIDTbiwpDn1OO/74Mvc7D1QqFok7mMTUC/FQsIARDFrqmxV3hCF1QwwjP0hlltXg2GViJvjpbXZ/3Terdt2ewlokTkGqqECzb365g4gkIRVAOFaq59gxeCmWwAinWdlNFI0xGeMh7WkqcEiVl05/yawjrQysIJK6BFhT9fdEikOlJqGvO/Mb1byXi/95vQSCcy9lIk6ACjJbFCTcgsjKg7EGTFICfKIJJpLpWy0ywhIT0PGVdQjO/MuLpH1Sd+y6c3NabVwUcZTQIaqgGnLQGWqgK9RELUTQI3pGr+jNeDJejHfjY9a6ZBQzB+gPjM8fyoWbgA==</latexit>

FIG. 4. The predicting reservoir replaces the external input of the listening reservoir with the post-processed reservoir output. The time increment τ in our discussion represents the amount of time for information to travel once around the feedback loop.

Details of our reservoir implementation are as follows. The reservoir dimension is N =
2000, and we use γ = 10. The N -by-N adjacency matrix M is chosen randomly with
sparse Erd¨os-Renyi connectivity and spectral radius 0.9; speciﬁcally, each element is chosen
independently to be nonzero with probability 0.02, nonzero elements are chosen uniformly between −1 and 1, and the resulting matrix is rescaled so that the magnitude of its largest
eigenvalue is 0.9. The N -by-3 matrix Win is chosen randomly so that each row has one non-zero element, chosen uniformly between −1 and 1. We evolve the Lorenz system and the listening reservoir (9) from time t = −100 to t = 60, and we discard 100 time units of transient evolution, so that training is based on u(t) and r(t) for 0 ≤ t ≤ 60. For training,
we constrain the 3-by-2N matrix Wout to have only 3N nonzero elements, namely the ﬁrst N elements of its ﬁrst two rows, and the ﬁrst N/2 and last N/2 elements of its third row.
(Thus, we ﬁt the x and y coordinates of the Lorenz state with linear functions of r, and the
z coordinate with a linear combination of the ﬁrst N/2 coordinates of r and the squares
of the second N/2 coordinates; for the Lorenz system, this is advantageous over using a purely linear function of r8.) Subject to this constraint, we select Wout so as to minimize the error function

3000

Woutq(r(0.02k)) − u(0.02k) 2 + β Wout 2;

(11)

k=1

here we have coarsely sampled the training data every 0.02 time units in order to reduce the amount of computation required by the regression. The second term in the error function modiﬁes ordinary linear least-squares regression in order to discourage overﬁtting; this modiﬁcation is often called ridge regression or Tikhonov regularization. Below, we will show results with regularization parameter β = 10−6 and with β = 0 (no regularization). We begin prediction by initializing ˆr(T ) = r(T ) and evolving the predicting reservoir (10), where T = 60 is the end of the listening and training periods.
In Fig. 5, we show the actual z(t) from a trajectory of the Lorenz system, and predictions zˆ(t) from two reservoirs that are identical except for their input strength parameter values [σ = 0.012 for Fig. 5(a) and σ = 0.014 for Fig. 5(b)]. Each reservoir is trained with the same Lorenz trajectory and with regularization parameter β = 10−6. Both reservoirs predict the short-term future similarly well, but for larger values of the prediction time t − T , only the second prediction continues with a Lorenz-like climate. We compare the two climate predictions over a longer time period in Fig. 6, which shows Poincar´e return maps of successive z(t) maxima. In Fig. 6(a), the red dots (showing the reservoir prediction) initially are near the blue dots (representing the Lorenz attractor), but eventually the red dots approach a period two orbit, indicated by the arrows. The large distance of the upper left arrow from the blue dots indicates that this period two orbit for the reservoir is not on

10

(a) 50

40

30

z

20

10

00

10

20

30

40

t-T

(b) 50

40

30

z

20

10

00

10

20

30

40

t-T

FIG. 5. Predicted (red) and actual (blue) z(t) for a chaotic Lorenz system trajectory, using the same randomly-generated reservoir with diﬀerent input strengths σ = 0.012 [panel (a)] and σ = 0.014 [panel (b)]. Both predictions remain well correlated with the actual trajectory for roughly 10 time units. After decorrelation, the ﬁrst prediction approaches a periodic orbit, whereas the second prediction appears to continue with a climate similar to that of the actual trajectory.

the Lorenz attractor. In contrast, the red dots in Fig. 6(b) remain near the blue dots at all times, indicating that the reservoir replicates the climate in the long term.

n+1 z max
n+1 z max

(a) 45

(b) 45

40

40

35

35

3030

35

40

45

zn max

3030

35

40

45

zn max

FIG. 6. Poincar´e return map of successive local maxima of z(t) for the actual (blue) and predicted
(red) trajectories for t − T from 0 to 300, using the same Lorenz trajectory and reservoir as Fig. 5, again with σ = 0.012 [panel (a)] and σ = 0.014 [panel (b)]. Here zmn ax represents the nth local maximum of z(t). The ﬁrst prediction approaches a period two orbit (indicated by the arrows) that
is not on the Lorenz attractor whereas the second prediction remains close to the Lorenz attractor.

Based on the arguments in Sec. II A, we hypothesize that for both σ = 0.012 and σ =
0.014, the listening reservoir (9) evolves toward a set φσ(A), where A is the Lorenz attractor and φσ is a generalized synchronization function. Our choice of spectral radius 0.9 for the adjacency matrix M is consistent with common practice in reservoir computing32, though it does not guarantee uniform contraction for the listening reservoir23. However, it does
guarantee that the eigenvalues of the Jacobian matrix of the right side of (9), evaluated at r = u = 0, have real parts at most γ(−1 + 0.9) = 10(−0.1) = −1. This suggests an asymptotic contraction rate of −1 or faster for the listening reservoir, and that after discarding 100 transient time units, r(t) is extremely close to φσ(A) for t ≥ 0.
Based on the arguments in Sec. II C, we hypothesize that the set φσ(A) is approximately invariant for the predicting reservoir (10). Based on the results in Figs. 5 and 6, we hy-
pothesize further that for σ = 0.014, there is an attracting invariant set for the predicting
reservoir near φσ(A), but that between σ = 0.014 and σ = 0.012, there is a bifurcation

11

that causes this invariant set either to become unstable or to be destroyed entirely. To corroborate this hypothesis, we compute the Lyapunov exponents of the predicting reservoir for an approximate trajectory on φσ(A), as described in Sec. II E.

Lyapunov exponents

5

4

3

2

1

0

-1

0-.2004

0.008

0.0σ12

0.016

0.02

FIG. 7. The three largest Lyapunov exponents of the predicting reservoir (10) on the invariant set
φσ(A) for the listening reservoir (9), as a function of the input strength σ, for the same reservoir as Figs. 5 and 6. Two exponents that are approximately constant as a function of σ, and which
approximate the two largest Lyapunov exponents of the Lorenz attractor, are colored red and blue;
the more variable exponent, which we call the transverse Lyapunov exponent and which determines
climate stability, is colored green. For values of σ for which we detect divergence from the Lorenz climate, we graph with a black dot the observed divergence rate λ∗, computed as described in the
text.
Fig. 7 shows the three largest Lyapunov exponents of the predicting reservoir (10) as the input strength σ varies from 0.004 to 0.02. We do not change the matrices M and Win, but for each value of σ, we perform a separate training (with β = 10−6 as before), resulting in a diﬀerent output weight matrix Wout. The exponents colored red and blue approximate the positive and zero Lyapunov exponents of the Lorenz attractor A (the approximation is closest for σ ≥ 0.01). Reproduction of the positive exponent of A in the reservoir dynamics on φσ(A) is a necessary consequence of successful attractor reconstruction, and does not indicate instability of φσ(A) to transverse perturbations. The exponent colored green estimates the largest of the transverse Lyapunov exponents described in Sec. II D. This exponent passes through zero, indicating a bifurcation, at σ ≈ 0.013.
Next, we compare the change in stability indicated by the computed transient Lyapunov exponent to a more direct computation indicating success or failure of climate replication. To detect when the prediction uˆ(t) = ψˆ (ˆr(t)) of the Lorenz state diverges from the true Lorenz attractor, we let ∆(t) be the Euclidean distance between the vector ﬁeld duˆ/dt implied by the predicting reservoir and the vector ﬁeld (right-hand side) of the Lorenz system (8), evaluated at [x, y, z]T = uˆ(t). [We calculate the reservoir-implied vector ﬁeld by the chain rule duˆ/dt = Dψˆ (ˆr(t))dˆr/dt, where Dψˆ is the Jacobian matrix of ψˆ = Woutq, and dˆr/dt is given by Eq. (10).] For each value of σ depicted in Fig. 7, we calculate the vector ﬁeld discrepancy ∆(t) for the prediction time period t ≥ T . If ∆(t) does not exceed a threshold value 20 for a duration of 800 time units, we consider the climate to be approximately reproduced. (Our threshold value 20 is small compared to the typical magnitude of the Lorenz vector ﬁeld.) Otherwise, we say that the prediction has “escaped” from the Lorenz attractor. In Fig. 7, we show a black dot at each value of σ for which we detect escape; these values are the same as those for which the computed transverse Lyapunov exponent is positive. The height of each black dot represents an observed divergence rate λ∗, computed as follows.
When we detect escape for a particular value of σ, we reinitialize the predicting reservoir (10) using ˆr(t0) = r(t0) for 1000 diﬀerent values of t0 ≥ T , where the values of r(t0) are determined by continuing to run the listening reservoir (9) for t ≥ T . For each t0, we evolve the predicting reservoir until the ﬁrst time t1 for which ∆(t1) ≥ 20, or until t1 − t0 = 800, whichever comes ﬁrst. If divergence from the attractor is governed by Lyapunov exponent λ, we should have ∆(t1) ≈ ∆(t0) exp(λ(t1 − t0)) in a certain average sense. We compute

12

the observed exponential divergence rate λ∗ = ln[∆(t1)/∆(t0)] / t1 − t0 , where the angle brackets represent an average over the 1000 values of t0. The computed values of λ∗ are shown as black dots in Fig. 7. The approximate agreement of λ∗ with the green curve (especially for 0.01 ≤ σ ≤ 0.013) demonstrates that the computed transverse Lyapunov exponent reﬂects divergence of predictions from the Lorenz attractor.
4

Lyapunov exponents

2

0

-2

-4 0.1 0.11

σ 0.12 0.13 0.14

0.15 0.16 0.17 0.18

FIG. 8. The three largest Lyapunov exponents of the predicting reservoir (10), and the estimated divergence rate λ∗, as a function of σ, using the same color scheme as Fig. 7. Here we use a diﬀerent
randomly-generated reservoir than in Fig. 7, and no regularization (β = 0) in the training.

To illustrate the correspondence between the computed transverse Lyapunov exponent and the observed divergence rates in a case where their dependence on σ is more complicated, we show in Fig. 8 the analogue of Fig. 7 in a case where no regularization (β = 0) is used in the training. Again, we see precise correspondence between detected failure of climate replication (presence of a black dot) and positive values of the transverse Lyapunov exponent (green curve), and good agreement with the observed divergence rates for these values of σ. In this case, there are two bifurcations, one near σ = 0.12 and one near σ = 0.16.
We remark that when we use regularization (β = 10−6) in the training, we do not observe as complicated a dependence of the computed transverse Lyapunov exponent on the input strength σ as in Fig. 8. Instead, the computed transverse Lyapunov exponent is typically negative and slowly varying across a wide range of σ values, for which climate replication is successful. In Fig. 9, we use the transverse Lyapunov exponent computation, averaged over 10 diﬀerent randomly-generated reservoirs, to give a quantitative illustration of the advantage of regularization. When regularization is used, the negative means and small standard deviations of the computed transverse Lyapunov exponent indicate robust climate stability over the entire range 0.05 ≤ σ ≤ 0.5. (By contrast, Figs. 5–7 depicted values of σ ≤ 0.02.) With no regularization, the means are larger and more variable, indicating less stability and greater sensitivity to the value of σ, and the standard deviations are signiﬁcantly larger, indicating lack of robustness from one random reservoir realization to another.

IV. CONCLUSIONS AND DISCUSSION
We presented in Sec. II a partial explanation for how reservoir computing prediction is able to reconstruct the attractor (replicate the climate) for a chaotic process from limited time series data. We argued that the reservoir dynamics (2) can be designed so that during the listening period on which training is based, the reservoir state r(t) is approximately a continuous function φ of the state s(t) of the chaotic process. This property, called generalized synchronization, is closely related to the echo state property for reservoir computing. We showed that both properties hold if the listening reservoir (2) is uniformly contracting as a function of the reservoir state; other criteria for these properties have also been identiﬁed23,26,32.

13

(a)10

Lyapunov exponents

5

0

-5

σ 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45

0.5

(b)10

Lyapunov exponents

5

0

-5

σ 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45

0.5

FIG. 9. The means and standard deviations of three largest Lyapunov exponents for the same 10 randomly-generated reservoirs trained with regularization parameter β = 10−6 [panel (a)] and
with β = 0 [panel (b)]. Again, the red and blue curves approximate the two largest exponents of
the Lorenz attractor, and the green curve is the computed transverse Lyapunov exponent.

Ideally, the synchronization function φ should be one-to-one in order to recover the process dynamics from the reservoir dynamics. Investigation of conditions that can guarantee φ to be one-to-one could help guide reservoir design. However, even in the absence of a guarantee, we noted that embedding results suggest that φ is likely to be one-to-one if the reservoir state space is suﬃciently high-dimensional compared with dimensionality of the chaotic process.
Practically speaking, a necessary condition for climate replication is that training be successful in approximately recovering the measured state u(t) = h(s(t)) from the reservoir state r(t); this depends on the amount of training data available and the method of regression used, among other things. We did not address theoretical aspects of training, but we argued that success is plausible if the reservoir is suﬃciently high-dimensional and heterogeneous to yield a large variety of basis functions for the regression.
We showed that in the limit that the approximations we described are exact, the predicting reservoir (4) exactly predicts future values of u(t). Thus, accurate approximations yield commensurately accurate short-term forecasts. Long-term climate replication depends on stability of the predicting reservoir dynamics with respect to perturbations produced by the approximations. We discussed how to estimate Lyapunov exponents for the predicting reservoir in numerical experiments, whether or not the desired climate is stable. We emphasize that our computation of Lyapunov exponents was intended to illustrate our theory, and that the method we described requires measurements {u(t)} over a long time period to maintain the desired climate. If one’s goal is to estimate the Lyapunov exponents of the process that produced {u(t)} from a limited amount of data, one should seek parameters of the predicting reservoir that replicate the climate, and simply compute the Lyapunov exponents of the resulting trajectory8.
In Sec. III, we gave examples of climate replication successes and failures, and showed how they correspond to the Lyapunov exponents we computed. We emphasize that the results and the ranges of σ we displayed were selected to illustrate and analyze failures that can occur with inadequate input strength (Figs. 5–7) or without regularization (Fig. 8) in the training. With regularization, we are able to obtain robust climate replication [indicated

14
by Fig. 9(a)] over a wide range of input strengths.
We remark that for simplicity, our theory considered discrete-time reservoir dynamics. Discrete time is the appropriate way to model software reservoirs, but physical reservoirs typically are better modeled by continuous time. With appropriate modiﬁcations, our theory applies to the continuous-time case. The prediction time increment τ used in the training should be the amount of time information takes to traverse the feedback loop depicted in Fig. 4. However, with a physical reservoir, careful calibration of the sampled training data may be necessary to meet the goal of predicting u(t+τ ) based on the listening reservoir’s response to input up to time t, in part because τ is a property of the predicting reservoir and not of the listening reservoir.
Finally, we argue that in addition to reservoir computing, the theory we presented in Section II applies to some other machine learning methods for time series prediction. The essential features a prediction method needs for our theory to apply are: (1) that the method maintains an internal state, or “memory”, that depends on the sequence of inputs it receives during training; (2) that it is trained to predict a short time increment ahead, after receiving the input time series for a relatively long time interval; and (3) that it is used to predict farther into the future by iterating its incremental forecasts through a feedback loop. These features are present, for example, in prediction using the FORCE method for training reservoirs33 and in recent work using long short-term memory (LSTM) networks for prediction18. For methods that (unlike reservoir computing) train parameters that aﬀect the internal state in the absence of feedback, our theory applies if we take the function f in Eq. (2) to represent the update rule for the internal state r after training has selected parameter values. Though our description of how training arrives at the pair of functions (f ,ψˆ ) was speciﬁc to reservoir computing, our discussion of how these functions can be used with Eqs. (4) and (6) for prediction and attractor reconstruction are independent of which machine-learning method is used to determine the functions.
We gratefully acknowledge the support of grants from ARO (W911NF-12-1-0101) and DARPA. We thank Michelle Girvan, Jaideep Pathak, Sarthak Chandra, Daniel Gauthier, and Daniel Canaday for their input.
1H. Jaeger, German National Research Center for Information Technology: GMD Technical Report 148 (2001).
2W. Maass, T. Natschl¨ager, and H. Markram, Neural Computation 14, 2531 (2002). 3H. Jaeger, Scholarpedia 2, 2330 (2007). 4M. Lukoˇseviˇcius and H. Jaeger, Computer Science Review 3, 127 (2009). 5H. Jaeger and H. Haas, Science 304, 78 (2004). 6U. Parlitz and A. Hornstein, Chaos and Complexity Letters 1, 135 (2005). 7F. Wyﬀels and B. Schrauwen, Neurocomputing 73, 1958 (2010). 8J. Pathak, Z. Lu, B. R. Hunt, M. Girvan, and E. Ott, Chaos 27, 121102 (2017). 9Z. Lu, J. Pathak, B. Hunt, M. Girvan, R. Brockett, and E. Ott, Chaos 27, 041102 (2017). 10R. S. Zimmermann and U. Parlitz, Chaos 28, 043118 (2018). 11C. Fernando and S. Sojakka, in European Conference on Artiﬁcial Life (Springer, 2003) pp. 588–597. 12L. Appeltant, M. C. Soriano, G. Van der Sande, J. Danckaert, S. Massar, J. Dambre, B. Schrauwen, C. R.
Mirasso, and I. Fischer, Nature Communications 2, 468 (2011). 13N. D. Haynes, M. C. Soriano, D. P. Rosin, I. Fischer, and D. J. Gauthier, Physical Review E 91, 020801
(2015). 14D. Brunner, S. Reitzenstein, and I. Fischer, in Rebooting Computing (ICRC), IEEE International Con-
ference on (IEEE, 2016) pp. 1–2. 15L. Larger, A. Bayl´on-Fuentes, R. Martinenghi, V. S. Udaltsov, Y. K. Chembo, and M. Jacquot, Physical
Review X 7, 011015 (2017). 16Y. LeCun, Y. Bengio, and G. Hinton, Nature 521, 436 (2015). 17I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning (MIT press, 2016). 18P. R. Vlachas, W. Byeon, Z. Y. Wan, T. P. Sapsis, and P. Koumoutsakos, Proceedings of the Royal
Society A 474, 20170844 (2018). 19V. Afraimovich, N. Verichev, and M. I. Rabinovich, Radiophysics and Quantum Electronics 29, 795
(1986). 20L. M. Pecora and T. L. Carroll, Physical Review Letters 64, 821 (1990). 21N. F. Rulkov, M. M. Sushchik, L. S. Tsimring, and H. D. Abarbanel, Physical Review E 51, 980 (1995). 22L. Kocarev and U. Parlitz, Physical Review Letters 76, 1816 (1996). 23I. B. Yildiz, H. Jaeger, and S. J. Kiebel, Neural Networks 35, 1 (2012).

15
24M. Hirsch and C. Pugh, in Global Analysis, Proceedings of Symposia in Pure Mathematics, Vol. 14 (American Mathematical Society, 1970) pp. 133–164.
25M. Hirsch and C. Pugh, Invariant Manifolds, Lecture Notes in Mathematics No. 583 (Springer-Verlag, 1977).
26J. Stark, Physica D 64, 163 (1997). 27H. Whitney, Annals of Mathematics 37, 645 (1936). 28T. Sauer, J. A. Yorke, and M. Casdagli, Journal of Statistical Physics 65, 579 (1991). 29J. Pathak, A. Wikner, R. Fussell, S. Chandra, B. R. Hunt, M. Girvan, and E. Ott, Chaos 28, 041101
(2018). 30B. R. Hunt, E. Ott, and J. A. Yorke, Physical Review E 55, 4029 (1997). 31E. N. Lorenz, Journal of the Atmospheric Sciences 20, 130 (1963). 32K. Caluwaerts, F. Wyﬀels, S. Dieleman, and B. Schrauewn, in 2013 International Joint Conference on
Neural Networks (IEEE, 2014) pp. 1–6. 33D. Sussillo and L. F. Abbott, Neuron 63, 544 (2009).

